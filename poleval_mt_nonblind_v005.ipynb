{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "poleval_mt_nonblind_v005.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "256a94d9538a47bb8ceef6339c78a526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d199499d609d4224aa7f3e8a3a3f884a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b508a7730b744e31ad5ab91705dbe355",
              "IPY_MODEL_40c771f107e1462dbda578a132f45185",
              "IPY_MODEL_51a5c9d937744cbf86b1163516f09325"
            ]
          }
        },
        "d199499d609d4224aa7f3e8a3a3f884a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b508a7730b744e31ad5ab91705dbe355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_82dd8880649b4ad38b156c213af5562a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_31ff451903e944138604e794b69a0af0"
          }
        },
        "40c771f107e1462dbda578a132f45185": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6cefe003dedf4249ac76d689647753ab",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_66fc6747f49e407091bd3dcc73672394"
          }
        },
        "51a5c9d937744cbf86b1163516f09325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_542c4a7c62da4dde867e31bbe553aa00",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00,  6.10ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2510c175b5404d55a7941450b887e45b"
          }
        },
        "82dd8880649b4ad38b156c213af5562a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "31ff451903e944138604e794b69a0af0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6cefe003dedf4249ac76d689647753ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "66fc6747f49e407091bd3dcc73672394": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "542c4a7c62da4dde867e31bbe553aa00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2510c175b5404d55a7941450b887e45b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ad7087f3624f43d2aebe6577490acbe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dfbd14fdc1ea4e6bad0c2c1e36223349",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_748ad0ec8e7e45aaaeccfa2583423f70",
              "IPY_MODEL_643b2be43fa141678d25c49f13b9713a",
              "IPY_MODEL_281ea4ca39d7470cbbe10a047fb4ab0e"
            ]
          }
        },
        "dfbd14fdc1ea4e6bad0c2c1e36223349": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "748ad0ec8e7e45aaaeccfa2583423f70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a050da21b52c4b5b86294bf2483f9425",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_33abdc0e6b454ce69a938a5479ff4109"
          }
        },
        "643b2be43fa141678d25c49f13b9713a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_251179bb71b546d1bf444dcc09136c8a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3d947f420ad940c78e0f5e78cdf07e62"
          }
        },
        "281ea4ca39d7470cbbe10a047fb4ab0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ec03ddbcf3bb4246bf0aa436cd86feb8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00,  7.23ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d79da2458910462793613161da8612ac"
          }
        },
        "a050da21b52c4b5b86294bf2483f9425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "33abdc0e6b454ce69a938a5479ff4109": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "251179bb71b546d1bf444dcc09136c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3d947f420ad940c78e0f5e78cdf07e62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec03ddbcf3bb4246bf0aa436cd86feb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d79da2458910462793613161da8612ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "336009fba0564e1fbe214a63c1d78815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e2a3eb9f4a724361bd5df5cbb8fd7158",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0191be4e9319453e9752eef645607204",
              "IPY_MODEL_b9aae9636f6c4887894c5ab6fd0b0d5d",
              "IPY_MODEL_27b4c76e6b334ddfa24a0cb903cd3a4d"
            ]
          }
        },
        "e2a3eb9f4a724361bd5df5cbb8fd7158": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0191be4e9319453e9752eef645607204": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_daadc464913e46d5b18a905077e6dcbb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9a2be43734734e258579380f2a5d1f88"
          }
        },
        "b9aae9636f6c4887894c5ab6fd0b0d5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_75607fc160b849ae92220f8895e16450",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fe5f9d81cd2c45b6baa2f34394c20dd0"
          }
        },
        "27b4c76e6b334ddfa24a0cb903cd3a4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cda1fa33d44f4ad7a3bf9966bbad99a2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00, 14.81ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_008de809931347ae8fc4a0347c14e474"
          }
        },
        "daadc464913e46d5b18a905077e6dcbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9a2be43734734e258579380f2a5d1f88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "75607fc160b849ae92220f8895e16450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fe5f9d81cd2c45b6baa2f34394c20dd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cda1fa33d44f4ad7a3bf9966bbad99a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "008de809931347ae8fc4a0347c14e474": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "48a946c4d1c7416d8911a43a5cf214f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ce7a8af300bc48f59fc7695bb40c812c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ff92a72770724d92846b930f2a8a01d7",
              "IPY_MODEL_6554fbe963da414fb676a635ed62d2df",
              "IPY_MODEL_874d56f56ad34252bc270b9146e0755d"
            ]
          }
        },
        "ce7a8af300bc48f59fc7695bb40c812c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ff92a72770724d92846b930f2a8a01d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dfcf15870d3f414b850811986a49a45f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_21130ef2fb8541339f81d25f148d7b80"
          }
        },
        "6554fbe963da414fb676a635ed62d2df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1aa97ecbdbe74365b4d1091a8b838124",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a5e35a40f344d1dbf968edcffa3f818"
          }
        },
        "874d56f56ad34252bc270b9146e0755d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6c2e9a95c97b4bf2b9b110ad9b266b97",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00,  6.04ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2bd98eb0f7c9438885b05361982ad08b"
          }
        },
        "dfcf15870d3f414b850811986a49a45f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "21130ef2fb8541339f81d25f148d7b80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1aa97ecbdbe74365b4d1091a8b838124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a5e35a40f344d1dbf968edcffa3f818": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c2e9a95c97b4bf2b9b110ad9b266b97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2bd98eb0f7c9438885b05361982ad08b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cafb627ffd95461d94fd5576eb335f09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_020b9f22f15a411cbd6e4dacdd63f727",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a56aff7b974b4e7da00251b9f275b821",
              "IPY_MODEL_0fd73bd4dae647fba1315ef7551856db",
              "IPY_MODEL_039bfac9a5fe4ced865d7c311ef49095"
            ]
          }
        },
        "020b9f22f15a411cbd6e4dacdd63f727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a56aff7b974b4e7da00251b9f275b821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b109050cf4044671a1fe57583f99d4dd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_57a713c6c0434b21835ed54642dbcc16"
          }
        },
        "0fd73bd4dae647fba1315ef7551856db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_17e03cc821f44a0cbf9876f24628f502",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5bbf2f3d11d1470a8769a55249693909"
          }
        },
        "039bfac9a5fe4ced865d7c311ef49095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0f664b4df17641e09dbd4e80dfac152c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00, 10.06ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0ed0ab0805f2490880c107f903e3b589"
          }
        },
        "b109050cf4044671a1fe57583f99d4dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "57a713c6c0434b21835ed54642dbcc16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "17e03cc821f44a0cbf9876f24628f502": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5bbf2f3d11d1470a8769a55249693909": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0f664b4df17641e09dbd4e80dfac152c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0ed0ab0805f2490880c107f903e3b589": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "022e904b4265497bbe41d22a5f23a12b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_13c90fb95a10410da42d485496b893a9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_094110dbfab7420892f802c641d96b7b",
              "IPY_MODEL_76d7eaa8934a4d89bd583498ddd12703",
              "IPY_MODEL_b271c43f3c074ce294e4f0c16cd08ed2"
            ]
          }
        },
        "13c90fb95a10410da42d485496b893a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "094110dbfab7420892f802c641d96b7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_83ac80c7c40c4232850f5a7b307c65dd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9acd574b4a924d19aadcdb615d72cc7d"
          }
        },
        "76d7eaa8934a4d89bd583498ddd12703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ab704ea649884eaf9e99b4f81817f8de",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7f6bd752a1c24bfeb35c7006c62c19d5"
          }
        },
        "b271c43f3c074ce294e4f0c16cd08ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_096d5ce95b5044acb50a3f1ce1334b52",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00,  6.39ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_696b6a149812441b921d958401d2f295"
          }
        },
        "83ac80c7c40c4232850f5a7b307c65dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9acd574b4a924d19aadcdb615d72cc7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab704ea649884eaf9e99b4f81817f8de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7f6bd752a1c24bfeb35c7006c62c19d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "096d5ce95b5044acb50a3f1ce1334b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "696b6a149812441b921d958401d2f295": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f58f380f8244147a296dd2b973d540c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5d2c18a87f094050bea3c657809c0cfc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0f7d0dc018954b3ca55fdca5f1eb2ee4",
              "IPY_MODEL_45e91ff001a44eac8ca90e5916b8e709",
              "IPY_MODEL_4321fa5e491045c28e48357936fa0b74"
            ]
          }
        },
        "5d2c18a87f094050bea3c657809c0cfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0f7d0dc018954b3ca55fdca5f1eb2ee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e4f0f4eb8fc142b6a6f7e0f967dcc1b6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d323c6f564af45cea6e3f0052e40867f"
          }
        },
        "45e91ff001a44eac8ca90e5916b8e709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_af9baca76eb7467687384f97c9043ccc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_68e6ee27d22f4d3abd58441fe27c38be"
          }
        },
        "4321fa5e491045c28e48357936fa0b74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ae49866e47e74c578f5ecf8672a9bd70",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00,  8.53ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9912694ff6ab4125854911cda32ff78c"
          }
        },
        "e4f0f4eb8fc142b6a6f7e0f967dcc1b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d323c6f564af45cea6e3f0052e40867f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af9baca76eb7467687384f97c9043ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "68e6ee27d22f4d3abd58441fe27c38be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ae49866e47e74c578f5ecf8672a9bd70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9912694ff6ab4125854911cda32ff78c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "561877ff5d6444bc8643db7718ba8ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2f8b510d2e924444b0376779f4645c0f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7d11acc6b3df43be8f246842c948bb98",
              "IPY_MODEL_fb8e44e8cde64d95b3d3c5a2fbbb8352",
              "IPY_MODEL_d973937cf72e49af928d103debd64fe5"
            ]
          }
        },
        "2f8b510d2e924444b0376779f4645c0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d11acc6b3df43be8f246842c948bb98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b89ebad05ea245f490437a41d9355878",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d9986a72895e4b938bee89ac8147dd77"
          }
        },
        "fb8e44e8cde64d95b3d3c5a2fbbb8352": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6cf6194af4ef45609b775439f3828bb6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_634dbf5013ed46caa4ff5a98f1d63433"
          }
        },
        "d973937cf72e49af928d103debd64fe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6799baf1e77a4b7c8b96c00d7506d51d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00,  5.16ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_698d42ccb57f4d2dbf35d461f06ddfb6"
          }
        },
        "b89ebad05ea245f490437a41d9355878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d9986a72895e4b938bee89ac8147dd77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6cf6194af4ef45609b775439f3828bb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "634dbf5013ed46caa4ff5a98f1d63433": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6799baf1e77a4b7c8b96c00d7506d51d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "698d42ccb57f4d2dbf35d461f06ddfb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a63aab736a43421bb2653fe7a3ae3d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6fca9fc98b914043b3ce4a657a3b07bc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_05ac9e0e745d4a43abd050acfe3c9269",
              "IPY_MODEL_6c2c910152254a2d929b0cf0be9e2a99",
              "IPY_MODEL_9441497a64fe4334b67f2047ef9719d3"
            ]
          }
        },
        "6fca9fc98b914043b3ce4a657a3b07bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "05ac9e0e745d4a43abd050acfe3c9269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ed41ccf260c948e1ad021cea0a8b4c35",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5d18cf28c85d422d84a7e7f1ea2bb842"
          }
        },
        "6c2c910152254a2d929b0cf0be9e2a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_160b0729d4174e8fb2bb454c982a33d2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f960048443e34ed1ad8cfa36d5391e8a"
          }
        },
        "9441497a64fe4334b67f2047ef9719d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_82bdd39368d347ad91668967d7cd41ec",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00,  8.57ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ada14087705a4d32b3349c03c211827b"
          }
        },
        "ed41ccf260c948e1ad021cea0a8b4c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5d18cf28c85d422d84a7e7f1ea2bb842": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "160b0729d4174e8fb2bb454c982a33d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f960048443e34ed1ad8cfa36d5391e8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "82bdd39368d347ad91668967d7cd41ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ada14087705a4d32b3349c03c211827b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "291e0568c6ad4d4d9d2cacbceb83efbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7e111a1191a64598a3c657263dc97086",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7e822acbb00448cfbd7c485568b7f6ec",
              "IPY_MODEL_9594004ccb2c45afafa5a0d2f9f07c93",
              "IPY_MODEL_84b02e63576047a891c56d6da9179ae0"
            ]
          }
        },
        "7e111a1191a64598a3c657263dc97086": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7e822acbb00448cfbd7c485568b7f6ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e8564bae129d4733977924fbf4a6f1a1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1a3eda4781e449a4903afd370148a749"
          }
        },
        "9594004ccb2c45afafa5a0d2f9f07c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e77edf17ba374fada50760f63a15de53",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4c02b1b591da4189a96c1efc7927fbda"
          }
        },
        "84b02e63576047a891c56d6da9179ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ace9167d783e407d8be4221daddbeb8b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00,  6.46ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ff9c145c30e844059c06992b33ca5fae"
          }
        },
        "e8564bae129d4733977924fbf4a6f1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1a3eda4781e449a4903afd370148a749": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e77edf17ba374fada50760f63a15de53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4c02b1b591da4189a96c1efc7927fbda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ace9167d783e407d8be4221daddbeb8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ff9c145c30e844059c06992b33ca5fae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd731a30265142e3a4dd596873f4f78a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_11ab0b69eae9495ba150eb0b9f49a307",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c68b1d353ca84e1bb41fef3cbd9d4f66",
              "IPY_MODEL_48cb95bac0ef4e6ea40bf129f1dcfc7b",
              "IPY_MODEL_5e52b025508a4ec28aead603a93e9225"
            ]
          }
        },
        "11ab0b69eae9495ba150eb0b9f49a307": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c68b1d353ca84e1bb41fef3cbd9d4f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7e183d801346434eb52f23bba2c07f54",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b4b5016d51304158951fb87581eed90d"
          }
        },
        "48cb95bac0ef4e6ea40bf129f1dcfc7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f27f67f11a3448ec9b55c76408b44452",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7cc9c3fe29be43bc8213a4f080df06a4"
          }
        },
        "5e52b025508a4ec28aead603a93e9225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e031ced25bb5401eba8283b018ce986a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00, 13.24ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9d183b0a79004661bb2a95e3d1234cb3"
          }
        },
        "7e183d801346434eb52f23bba2c07f54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b4b5016d51304158951fb87581eed90d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f27f67f11a3448ec9b55c76408b44452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7cc9c3fe29be43bc8213a4f080df06a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e031ced25bb5401eba8283b018ce986a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9d183b0a79004661bb2a95e3d1234cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek0QjYHhiou4"
      },
      "source": [
        "EXPNUM = '005'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GFgD0pVkcNH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14cf45d7-f15e-44ad-e7d7-659de6918b6b"
      },
      "source": [
        "!git clone --single-branch https://github.com/poleval/2021-quality-estimation-nonblind -b main\n",
        "!pip install datasets -qq\n",
        "!pip install transformers -qq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '2021-quality-estimation-nonblind'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 30 (delta 10), reused 26 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (30/30), done.\n",
            "\u001b[K     |████████████████████████████████| 264 kB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 118 kB 66.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 243 kB 58.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 50.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 47.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 37.6 MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTnj9Z6sICRY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "HzmWfdWKy-CY",
        "outputId": "b03900fb-b240-4a0a-fec9-7b7113dc8335"
      },
      "source": [
        "df1 = pd.read_csv('2021-quality-estimation-nonblind/dev-0/expected.tsv', sep='\\t', header=None, names=['score'])\n",
        "df2 = pd.read_csv('2021-quality-estimation-nonblind/dev-0/in.tsv', sep='\\t', header=None, names=['t1', 't2', 't3'])\n",
        "df3 = pd.read_csv('2021-quality-estimation-nonblind/test-A/expected.tsv', sep='\\t', header=None, names=['score'])\n",
        "df4 = pd.read_csv('2021-quality-estimation-nonblind/test-A/in.tsv', sep='\\t', header=None, names=['t1', 't2', 't3'])\n",
        "df5 = pd.concat([df2,df1], axis=1)\n",
        "df6 = pd.concat([df4,df3], axis=1)\n",
        "df = pd.concat([df5,df6], axis=0, ignore_index=True)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>t1</th>\n",
              "      <th>t2</th>\n",
              "      <th>t3</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Zdjęcie satelitarne spoglądające na ziemię.</td>\n",
              "      <td>A satellite picture looking down at the earth.</td>\n",
              "      <td>Zdjęcie satelitarne Ziemii.</td>\n",
              "      <td>4.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dlatego chcą zaistnieć jako potęga przemysłowa...</td>\n",
              "      <td>So they want to emerge as an industrial powerh...</td>\n",
              "      <td>Chcą przejawiać się jako siła napędzająca prze...</td>\n",
              "      <td>4.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I umieściłem je na moim zdjęciu.</td>\n",
              "      <td>And I put them into my photograph.</td>\n",
              "      <td>Umieściłam je na swoim zdjęciu.</td>\n",
              "      <td>4.583333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I znowu możesz myśleć o tym jak o prawie mapow...</td>\n",
              "      <td>And again you can think of this as almost mapp...</td>\n",
              "      <td>I znów można to traktować prawie jak mapowanie...</td>\n",
              "      <td>3.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Więc co jest najlepsze.</td>\n",
              "      <td>So what is the best.</td>\n",
              "      <td>Która metoda jest najlepsza?</td>\n",
              "      <td>4.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>980</th>\n",
              "      <td>Teraz pamiętaj, że ma iść do niebieskich kwiatów.</td>\n",
              "      <td>Now remember she is supposed to be going to th...</td>\n",
              "      <td>Teraz pamiętajcie  że ona miała iść do niebies...</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>981</th>\n",
              "      <td>Tak więc w drugiej wersji jest 100 miliardów l...</td>\n",
              "      <td>So in the second version of it there are 100 b...</td>\n",
              "      <td>Więc w drugiej wersji jest 100 miliardów czy 1...</td>\n",
              "      <td>4.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982</th>\n",
              "      <td>I nie zawsze ze sobą współpracują. Podoba mi s...</td>\n",
              "      <td>And they don not always work together I like F...</td>\n",
              "      <td>które nie zawsze współpracują. Lubię teorię Fr...</td>\n",
              "      <td>4.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>983</th>\n",
              "      <td>A jednak wciąż jest jeden na ciemnym tle, a dr...</td>\n",
              "      <td>And yet there is still one on a dark surround ...</td>\n",
              "      <td>Ale mimo wszystko jeden jest na ciemnym a drug...</td>\n",
              "      <td>4.583333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>984</th>\n",
              "      <td>Nie wiem, czy nadal możesz zobaczyć czerwoną l...</td>\n",
              "      <td>I don not know if you can still see the red li...</td>\n",
              "      <td>Nie wiem czy nadal widać tę czerwoną linię na ...</td>\n",
              "      <td>4.666667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>985 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    t1  ...     score\n",
              "0          Zdjęcie satelitarne spoglądające na ziemię.  ...  4.500000\n",
              "1    Dlatego chcą zaistnieć jako potęga przemysłowa...  ...  4.666667\n",
              "2                     I umieściłem je na moim zdjęciu.  ...  4.583333\n",
              "3    I znowu możesz myśleć o tym jak o prawie mapow...  ...  3.833333\n",
              "4                              Więc co jest najlepsze.  ...  4.416667\n",
              "..                                                 ...  ...       ...\n",
              "980  Teraz pamiętaj, że ma iść do niebieskich kwiatów.  ...  5.000000\n",
              "981  Tak więc w drugiej wersji jest 100 miliardów l...  ...  4.500000\n",
              "982  I nie zawsze ze sobą współpracują. Podoba mi s...  ...  4.666667\n",
              "983  A jednak wciąż jest jeden na ciemnym tle, a dr...  ...  4.583333\n",
              "984  Nie wiem, czy nadal możesz zobaczyć czerwoną l...  ...  4.666667\n",
              "\n",
              "[985 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER03iK1HuFrr"
      },
      "source": [
        "tst = pd.read_csv('2021-quality-estimation-nonblind/test-B/in.tsv', sep='\\t', header=None, names=['t1', 't2', 't3'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAf6PzrulUEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74f8b3d-3273-45d2-e3cd-8c0e58a437a4"
      },
      "source": [
        "def print_high_low(df):\n",
        "  h = df[df.score == df.score.max()].index[0]\n",
        "  l = df[df.score == df.score.min()].index[0]\n",
        "\n",
        "  rl = df.t3.loc[l]\n",
        "  rh = df.t3.loc[h]\n",
        "  tl = df.t1.loc[l]\n",
        "  th = df.t1.loc[h]\n",
        "\n",
        "  print(f'Worst translation:')\n",
        "  print(f'Translation: {tl}')\n",
        "  print(f'Reference: {rl}')\n",
        "  print(f'Best tranlation:')\n",
        "  print(f'Translation: {th}')\n",
        "  print(f'Reference: {rh}')\n",
        "\n",
        "print_high_low(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Worst translation:\n",
            "Translation: Innej rzeczy można się nauczyć, podróżując do tych krajów na Bliskim Wschodzie, czasami do krajów Ameryki Łacińskiej. Kraje Ameryki Południowej.\n",
            "Reference: Inna rzecz którą można zauważyć gdy podróżuje się po Bliskim Wschodzie czasem w Ameryce Łacińskiej Ameryce Południowej często kiedy budują robią to bez reguł i zasad.\n",
            "Best tranlation:\n",
            "Translation: Bum.\n",
            "Reference: Bum!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY2yWcCXsuQy"
      },
      "source": [
        "def create_folds(data, num_splits):\n",
        "    # we create a new column called kfold and fill it with -1\n",
        "    data[\"kfold\"] = -1\n",
        "    \n",
        "    # the next step is to randomize the rows of the data\n",
        "    data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # calculate number of bins by Sturge's rule\n",
        "    # I take the floor of the value, you can also\n",
        "    # just round it\n",
        "    num_bins = int(np.floor(1 + np.log2(len(data))))\n",
        "    \n",
        "    # bin targets\n",
        "    data.loc[:, \"bins\"] = pd.cut(\n",
        "        data[\"score\"], bins=num_bins, labels=False\n",
        "    )\n",
        "    \n",
        "    # initiate the kfold class from model_selection module\n",
        "    kf = model_selection.StratifiedKFold(n_splits=num_splits, random_state=42)\n",
        "    \n",
        "    # fill the new kfold column\n",
        "    # note that, instead of targets, we use bins!\n",
        "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n",
        "        data.loc[v_, 'kfold'] = f\n",
        "    \n",
        "    # drop the bins column\n",
        "    data = data.drop(\"bins\", axis=1)\n",
        "\n",
        "    # return dataframe with folds\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCG0buvBtAYi",
        "outputId": "6c9efea5-9b72-49f1-8a0b-96adccbdd831"
      },
      "source": [
        "df = create_folds(df, num_splits=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  % (min_groups, self.n_splits)), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d03Sin9Ft3zm",
        "outputId": "21d12ccd-5943-40db-a132-f1601dfb94f9"
      },
      "source": [
        "all_texts = pd.concat([df, tst], axis=0, ignore_index=True)\n",
        "len(all_texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1985"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_4IXLWauN5Q"
      },
      "source": [
        "model_checkpoint = 'allegro/herbert-large-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV7ALZCazY1t",
        "outputId": "fb6ca298-281b-464c-e604-86a75bc6609e"
      },
      "source": [
        "def ct_toks(row):\n",
        "  t1 = row[0]\n",
        "  t2 = row[1]\n",
        "  t = t1 + ' ' + t2\n",
        "  return len(tokenizer(t)['input_ids'])\n",
        "\n",
        "all_texts['length'] = all_texts[['t1', 't3']].apply(ct_toks, axis=1)\n",
        "all_texts.length.min(), all_texts.length.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 205)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "-28QNibN0Hnh",
        "outputId": "0cc52c99-51f4-4700-bd6a-4d44c9239392"
      },
      "source": [
        "all_texts['length'].hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2183df23d0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRkHZCqXvehY"
      },
      "source": [
        "df = df.rename(columns={'score':'label'}) # HF expects this column name to pick up the target column in trainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rewsy6aQxfGB",
        "outputId": "806d86ad-d552-49d0-d5a4-6652a8485be0"
      },
      "source": [
        "val = df[df.kfold == 0].reset_index(drop=True)\n",
        "val['pred'] = df[df.kfold != 0].label.mean()\n",
        "mean_squared_error(val.label.values, val.pred.values, squared=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3797685384577315"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml2Jzj7A0yTY"
      },
      "source": [
        "model_checkpoint, lr, bs, max_length, grad_acc, wd, schedule, epochs = ('allegro/herbert-large-cased', 2e-05, 8, 192, 1, 0.01, 'constant', 6)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "\n",
        "if 'RoBERTa' in model_checkpoint: \n",
        "  print('Adding pad token')\n",
        "  tokenizer.pad_token = 0\n",
        "\n",
        "if 'large' in model_checkpoint:\n",
        "  init_layers = [23]\n",
        "else:\n",
        "  init_layers = [11]\n",
        "\n",
        "def weights_init_custom(model):\n",
        "  dense_names = [\"query\", \"key\", \"value\", \"dense\"]\n",
        "  layernorm_names = [\"LayerNorm\"]\n",
        "  prms = model.bert.named_parameters() if 'RoBERTa' not in model_checkpoint else model.roberta.named_parameters()\n",
        "  for name, module in prms:\n",
        "      if any(f\".{i}.\" in name for i in init_layers):\n",
        "          if any(n in name for n in dense_names):\n",
        "              if \"bias\" in name:\n",
        "                  module.data.zero_()\n",
        "              elif \"weight\" in name:\n",
        "                  module.data.normal_(mean=0.0, std=model.config.initializer_range)\n",
        "          elif any(n in name for n in layernorm_names):\n",
        "              if \"bias\" in name:\n",
        "                  module.data.zero_()\n",
        "              elif \"weight\" in name:\n",
        "                  module.data.fill_(1.0)\n",
        "\n",
        "def tokenize(batch): return tokenizer(batch['t1'], batch['t3'], padding='max_length', truncation=True, max_length=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBUECHWLvwrx"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    return {\n",
        "        'rmse': mean_squared_error(pred.label_ids, pred.predictions, squared=False),\n",
        "    }\n",
        "\n",
        "def model_init():\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=1)\n",
        "    weights_init_custom(model)\n",
        "    model.dropout = nn.Dropout(p=0.0, inplace=False)\n",
        "    paramg = model.bert.encoder.layer if 'RoBERTa' not in model_checkpoint else model.roberta.encoder.layer\n",
        "    for l in paramg: \n",
        "      l.attention.self.dropout = nn.Dropout(p=0.0, inplace=False)\n",
        "      l.attention.output.dropout = nn.Dropout(p=0.0, inplace=False)\n",
        "      l.output.dropout = nn.Dropout(p=0.0, inplace=False)\n",
        "    return model\n",
        "\n",
        "def init_trainer(fold, seed):\n",
        "    train_dataset = Dataset.from_pandas(df[df.kfold != fold].reset_index(drop=True))\n",
        "    valid_dataset = Dataset.from_pandas(df[df.kfold == fold].reset_index(drop=True))\n",
        "    train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
        "    valid_dataset = valid_dataset.map(tokenize, batched=True, batch_size=len(valid_dataset))\n",
        "    args = TrainingArguments(\n",
        "        \"./tmp\",\n",
        "        evaluation_strategy = \"steps\",\n",
        "        eval_steps=10,\n",
        "        save_steps=10,\n",
        "        save_total_limit=20,\n",
        "        learning_rate=lr,\n",
        "        fp16=False,\n",
        "        per_device_train_batch_size=bs,\n",
        "        per_device_eval_batch_size=bs,\n",
        "        gradient_accumulation_steps=grad_acc,\n",
        "        lr_scheduler_type=schedule,\n",
        "        num_train_epochs=epochs,\n",
        "        seed=1,\n",
        "        weight_decay=wd,\n",
        "        load_best_model_at_end=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model_init=model_init,\n",
        "        args=args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=valid_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    return trainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ4t-pDUKKOe"
      },
      "source": [
        "preds = []\n",
        "results = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48,
          "referenced_widgets": [
            "256a94d9538a47bb8ceef6339c78a526",
            "d199499d609d4224aa7f3e8a3a3f884a",
            "b508a7730b744e31ad5ab91705dbe355",
            "40c771f107e1462dbda578a132f45185",
            "51a5c9d937744cbf86b1163516f09325",
            "82dd8880649b4ad38b156c213af5562a",
            "31ff451903e944138604e794b69a0af0",
            "6cefe003dedf4249ac76d689647753ab",
            "66fc6747f49e407091bd3dcc73672394",
            "542c4a7c62da4dde867e31bbe553aa00",
            "2510c175b5404d55a7941450b887e45b"
          ]
        },
        "id": "omKHsddIKKGG",
        "outputId": "43caa526-4477-431c-d8ba-bb93ca87c4bb"
      },
      "source": [
        "test_df = tst.rename(columns={'score':'label'})\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "256a94d9538a47bb8ceef6339c78a526",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ad7087f3624f43d2aebe6577490acbe0",
            "dfbd14fdc1ea4e6bad0c2c1e36223349",
            "748ad0ec8e7e45aaaeccfa2583423f70",
            "643b2be43fa141678d25c49f13b9713a",
            "281ea4ca39d7470cbbe10a047fb4ab0e",
            "a050da21b52c4b5b86294bf2483f9425",
            "33abdc0e6b454ce69a938a5479ff4109",
            "251179bb71b546d1bf444dcc09136c8a",
            "3d947f420ad940c78e0f5e78cdf07e62",
            "ec03ddbcf3bb4246bf0aa436cd86feb8",
            "d79da2458910462793613161da8612ac",
            "336009fba0564e1fbe214a63c1d78815",
            "e2a3eb9f4a724361bd5df5cbb8fd7158",
            "0191be4e9319453e9752eef645607204",
            "b9aae9636f6c4887894c5ab6fd0b0d5d",
            "27b4c76e6b334ddfa24a0cb903cd3a4d",
            "daadc464913e46d5b18a905077e6dcbb",
            "9a2be43734734e258579380f2a5d1f88",
            "75607fc160b849ae92220f8895e16450",
            "fe5f9d81cd2c45b6baa2f34394c20dd0",
            "cda1fa33d44f4ad7a3bf9966bbad99a2",
            "008de809931347ae8fc4a0347c14e474",
            "48a946c4d1c7416d8911a43a5cf214f2",
            "ce7a8af300bc48f59fc7695bb40c812c",
            "ff92a72770724d92846b930f2a8a01d7",
            "6554fbe963da414fb676a635ed62d2df",
            "874d56f56ad34252bc270b9146e0755d",
            "dfcf15870d3f414b850811986a49a45f",
            "21130ef2fb8541339f81d25f148d7b80",
            "1aa97ecbdbe74365b4d1091a8b838124",
            "6a5e35a40f344d1dbf968edcffa3f818",
            "6c2e9a95c97b4bf2b9b110ad9b266b97",
            "2bd98eb0f7c9438885b05361982ad08b",
            "cafb627ffd95461d94fd5576eb335f09",
            "020b9f22f15a411cbd6e4dacdd63f727",
            "a56aff7b974b4e7da00251b9f275b821",
            "0fd73bd4dae647fba1315ef7551856db",
            "039bfac9a5fe4ced865d7c311ef49095",
            "b109050cf4044671a1fe57583f99d4dd",
            "57a713c6c0434b21835ed54642dbcc16",
            "17e03cc821f44a0cbf9876f24628f502",
            "5bbf2f3d11d1470a8769a55249693909",
            "0f664b4df17641e09dbd4e80dfac152c",
            "0ed0ab0805f2490880c107f903e3b589",
            "022e904b4265497bbe41d22a5f23a12b",
            "13c90fb95a10410da42d485496b893a9",
            "094110dbfab7420892f802c641d96b7b",
            "76d7eaa8934a4d89bd583498ddd12703",
            "b271c43f3c074ce294e4f0c16cd08ed2",
            "83ac80c7c40c4232850f5a7b307c65dd",
            "9acd574b4a924d19aadcdb615d72cc7d",
            "ab704ea649884eaf9e99b4f81817f8de",
            "7f6bd752a1c24bfeb35c7006c62c19d5",
            "096d5ce95b5044acb50a3f1ce1334b52",
            "696b6a149812441b921d958401d2f295",
            "5f58f380f8244147a296dd2b973d540c",
            "5d2c18a87f094050bea3c657809c0cfc",
            "0f7d0dc018954b3ca55fdca5f1eb2ee4",
            "45e91ff001a44eac8ca90e5916b8e709",
            "4321fa5e491045c28e48357936fa0b74",
            "e4f0f4eb8fc142b6a6f7e0f967dcc1b6",
            "d323c6f564af45cea6e3f0052e40867f",
            "af9baca76eb7467687384f97c9043ccc",
            "68e6ee27d22f4d3abd58441fe27c38be",
            "ae49866e47e74c578f5ecf8672a9bd70",
            "9912694ff6ab4125854911cda32ff78c",
            "561877ff5d6444bc8643db7718ba8ed5",
            "2f8b510d2e924444b0376779f4645c0f",
            "7d11acc6b3df43be8f246842c948bb98",
            "fb8e44e8cde64d95b3d3c5a2fbbb8352",
            "d973937cf72e49af928d103debd64fe5",
            "b89ebad05ea245f490437a41d9355878",
            "d9986a72895e4b938bee89ac8147dd77",
            "6cf6194af4ef45609b775439f3828bb6",
            "634dbf5013ed46caa4ff5a98f1d63433",
            "6799baf1e77a4b7c8b96c00d7506d51d",
            "698d42ccb57f4d2dbf35d461f06ddfb6",
            "a63aab736a43421bb2653fe7a3ae3d0e",
            "6fca9fc98b914043b3ce4a657a3b07bc",
            "05ac9e0e745d4a43abd050acfe3c9269",
            "6c2c910152254a2d929b0cf0be9e2a99",
            "9441497a64fe4334b67f2047ef9719d3",
            "ed41ccf260c948e1ad021cea0a8b4c35",
            "5d18cf28c85d422d84a7e7f1ea2bb842",
            "160b0729d4174e8fb2bb454c982a33d2",
            "f960048443e34ed1ad8cfa36d5391e8a",
            "82bdd39368d347ad91668967d7cd41ec",
            "ada14087705a4d32b3349c03c211827b",
            "291e0568c6ad4d4d9d2cacbceb83efbe",
            "7e111a1191a64598a3c657263dc97086",
            "7e822acbb00448cfbd7c485568b7f6ec",
            "9594004ccb2c45afafa5a0d2f9f07c93",
            "84b02e63576047a891c56d6da9179ae0",
            "e8564bae129d4733977924fbf4a6f1a1",
            "1a3eda4781e449a4903afd370148a749",
            "e77edf17ba374fada50760f63a15de53",
            "4c02b1b591da4189a96c1efc7927fbda",
            "ace9167d783e407d8be4221daddbeb8b",
            "ff9c145c30e844059c06992b33ca5fae",
            "bd731a30265142e3a4dd596873f4f78a",
            "11ab0b69eae9495ba150eb0b9f49a307",
            "c68b1d353ca84e1bb41fef3cbd9d4f66",
            "48cb95bac0ef4e6ea40bf129f1dcfc7b",
            "5e52b025508a4ec28aead603a93e9225",
            "7e183d801346434eb52f23bba2c07f54",
            "b4b5016d51304158951fb87581eed90d",
            "f27f67f11a3448ec9b55c76408b44452",
            "7cc9c3fe29be43bc8213a4f080df06a4",
            "e031ced25bb5401eba8283b018ce986a",
            "9d183b0a79004661bb2a95e3d1234cb3"
          ]
        },
        "id": "8tklptYev4N8",
        "outputId": "4585a903-da59-4e0d-dba3-269cc290b6c2"
      },
      "source": [
        "for fold in range(5):\n",
        "    for seed in range(1):\n",
        "        print('#' * 100)\n",
        "        print(f'TRAINING FOLD {fold}')\n",
        "        print('#' * 100)\n",
        "        trainer = init_trainer(fold, seed)\n",
        "        trainer.train()\n",
        "        metrics = trainer.evaluate()\n",
        "        results.append({\n",
        "            'fold': fold,\n",
        "            'seed': seed,\n",
        "            'rmse': metrics['eval_rmse']\n",
        "        })\n",
        "        print('#' * 100)\n",
        "        print(f'FOLD {fold} SCORE: {metrics[\"eval_rmse\"]}')\n",
        "        print('#' * 100)\n",
        "        test_preds = trainer.predict(test_dataset)\n",
        "        preds.append(test_preds[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "####################################################################################################\n",
            "TRAINING FOLD 0\n",
            "####################################################################################################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad7087f3624f43d2aebe6577490acbe0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "336009fba0564e1fbe214a63c1d78815",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/allegro/herbert-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e512a74b9a525c868c0560254b3a65dc6309948f8cb99a8375429de673d28332.60289bb41d6aec457a093f9438a2c590beb41d64e7c830eac00dc56d3d49c5f6\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/allegro/herbert-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ca58839b8e4b1222703e13158ffeb3a5a7330260cbc39513f74710674d70268b.ad71128a5739887a02bfa6de2fa8768f86e02cd13d0c308873f4cdba254e4c7c\n",
            "Some weights of the model checkpoint at allegro/herbert-large-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.sso.sso_relationship.weight', 'cls.sso.sso_relationship.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "loading configuration file https://huggingface.co/allegro/herbert-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e512a74b9a525c868c0560254b3a65dc6309948f8cb99a8375429de673d28332.60289bb41d6aec457a093f9438a2c590beb41d64e7c830eac00dc56d3d49c5f6\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/allegro/herbert-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ca58839b8e4b1222703e13158ffeb3a5a7330260cbc39513f74710674d70268b.ad71128a5739887a02bfa6de2fa8768f86e02cd13d0c308873f4cdba254e4c7c\n",
            "Some weights of the model checkpoint at allegro/herbert-large-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.sso.sso_relationship.weight', 'cls.sso.sso_relationship.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running training *****\n",
            "  Num examples = 788\n",
            "  Num Epochs = 6\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 594\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='594' max='594' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [594/594 52:05, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.252317</td>\n",
              "      <td>0.502311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.128476</td>\n",
              "      <td>0.358435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.129490</td>\n",
              "      <td>0.359847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.113255</td>\n",
              "      <td>0.336534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.173371</td>\n",
              "      <td>0.416378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.117553</td>\n",
              "      <td>0.342860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.145360</td>\n",
              "      <td>0.381261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.121551</td>\n",
              "      <td>0.348642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.181939</td>\n",
              "      <td>0.426543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.115456</td>\n",
              "      <td>0.339788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.170486</td>\n",
              "      <td>0.412899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.108466</td>\n",
              "      <td>0.329342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.120449</td>\n",
              "      <td>0.347058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.158482</td>\n",
              "      <td>0.398098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.116980</td>\n",
              "      <td>0.342023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.113645</td>\n",
              "      <td>0.337113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.115127</td>\n",
              "      <td>0.339303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.126142</td>\n",
              "      <td>0.355165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.106512</td>\n",
              "      <td>0.326362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.114052</td>\n",
              "      <td>0.337716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.108667</td>\n",
              "      <td>0.329646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.106879</td>\n",
              "      <td>0.326924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.128997</td>\n",
              "      <td>0.359161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.109783</td>\n",
              "      <td>0.331335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.110634</td>\n",
              "      <td>0.332618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.127294</td>\n",
              "      <td>0.356782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.143221</td>\n",
              "      <td>0.378445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.109058</td>\n",
              "      <td>0.330240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.131720</td>\n",
              "      <td>0.362933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.110190</td>\n",
              "      <td>0.331949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.103527</td>\n",
              "      <td>0.321756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.112282</td>\n",
              "      <td>0.335086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.156201</td>\n",
              "      <td>0.395222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.116481</td>\n",
              "      <td>0.341293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.110421</td>\n",
              "      <td>0.332296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.139095</td>\n",
              "      <td>0.372954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.132210</td>\n",
              "      <td>0.363607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.156464</td>\n",
              "      <td>0.395555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.109236</td>\n",
              "      <td>0.330508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.104360</td>\n",
              "      <td>0.323047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.104715</td>\n",
              "      <td>0.323597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.120525</td>\n",
              "      <td>0.347168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.150703</td>\n",
              "      <td>0.388204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.108046</td>\n",
              "      <td>0.328704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.107192</td>\n",
              "      <td>0.327402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.113364</td>\n",
              "      <td>0.336696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.107511</td>\n",
              "      <td>0.327889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.103918</td>\n",
              "      <td>0.322363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.111736</td>\n",
              "      <td>0.334269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.145600</td>\n",
              "      <td>0.115380</td>\n",
              "      <td>0.339676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.145600</td>\n",
              "      <td>0.107770</td>\n",
              "      <td>0.328283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.145600</td>\n",
              "      <td>0.107229</td>\n",
              "      <td>0.327459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.145600</td>\n",
              "      <td>0.115435</td>\n",
              "      <td>0.339758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.145600</td>\n",
              "      <td>0.108173</td>\n",
              "      <td>0.328896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.145600</td>\n",
              "      <td>0.111458</td>\n",
              "      <td>0.333853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.145600</td>\n",
              "      <td>0.122359</td>\n",
              "      <td>0.349798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.145600</td>\n",
              "      <td>0.112971</td>\n",
              "      <td>0.336112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.145600</td>\n",
              "      <td>0.106100</td>\n",
              "      <td>0.325730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.145600</td>\n",
              "      <td>0.123620</td>\n",
              "      <td>0.351596</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-10\n",
            "Configuration saved in ./tmp/checkpoint-10/config.json\n",
            "Model weights saved in ./tmp/checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-10/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-20\n",
            "Configuration saved in ./tmp/checkpoint-20/config.json\n",
            "Model weights saved in ./tmp/checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-20/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-30\n",
            "Configuration saved in ./tmp/checkpoint-30/config.json\n",
            "Model weights saved in ./tmp/checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-30/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-40\n",
            "Configuration saved in ./tmp/checkpoint-40/config.json\n",
            "Model weights saved in ./tmp/checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-40/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-50\n",
            "Configuration saved in ./tmp/checkpoint-50/config.json\n",
            "Model weights saved in ./tmp/checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-50/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-60\n",
            "Configuration saved in ./tmp/checkpoint-60/config.json\n",
            "Model weights saved in ./tmp/checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-60/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-70\n",
            "Configuration saved in ./tmp/checkpoint-70/config.json\n",
            "Model weights saved in ./tmp/checkpoint-70/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-70/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-70/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-80\n",
            "Configuration saved in ./tmp/checkpoint-80/config.json\n",
            "Model weights saved in ./tmp/checkpoint-80/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-80/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-80/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-90\n",
            "Configuration saved in ./tmp/checkpoint-90/config.json\n",
            "Model weights saved in ./tmp/checkpoint-90/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-90/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-90/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-100\n",
            "Configuration saved in ./tmp/checkpoint-100/config.json\n",
            "Model weights saved in ./tmp/checkpoint-100/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-100/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-110\n",
            "Configuration saved in ./tmp/checkpoint-110/config.json\n",
            "Model weights saved in ./tmp/checkpoint-110/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-110/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-110/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-120\n",
            "Configuration saved in ./tmp/checkpoint-120/config.json\n",
            "Model weights saved in ./tmp/checkpoint-120/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-120/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-120/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-130\n",
            "Configuration saved in ./tmp/checkpoint-130/config.json\n",
            "Model weights saved in ./tmp/checkpoint-130/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-130/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-130/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-140\n",
            "Configuration saved in ./tmp/checkpoint-140/config.json\n",
            "Model weights saved in ./tmp/checkpoint-140/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-140/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-140/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-150\n",
            "Configuration saved in ./tmp/checkpoint-150/config.json\n",
            "Model weights saved in ./tmp/checkpoint-150/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-150/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-150/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-160\n",
            "Configuration saved in ./tmp/checkpoint-160/config.json\n",
            "Model weights saved in ./tmp/checkpoint-160/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-160/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-160/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-170\n",
            "Configuration saved in ./tmp/checkpoint-170/config.json\n",
            "Model weights saved in ./tmp/checkpoint-170/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-170/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-170/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-180\n",
            "Configuration saved in ./tmp/checkpoint-180/config.json\n",
            "Model weights saved in ./tmp/checkpoint-180/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-180/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-180/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-190\n",
            "Configuration saved in ./tmp/checkpoint-190/config.json\n",
            "Model weights saved in ./tmp/checkpoint-190/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-190/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-190/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-200\n",
            "Configuration saved in ./tmp/checkpoint-200/config.json\n",
            "Model weights saved in ./tmp/checkpoint-200/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-200/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-200/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-210\n",
            "Configuration saved in ./tmp/checkpoint-210/config.json\n",
            "Model weights saved in ./tmp/checkpoint-210/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-210/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-210/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-10] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-220\n",
            "Configuration saved in ./tmp/checkpoint-220/config.json\n",
            "Model weights saved in ./tmp/checkpoint-220/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-220/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-220/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-20] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-230\n",
            "Configuration saved in ./tmp/checkpoint-230/config.json\n",
            "Model weights saved in ./tmp/checkpoint-230/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-230/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-230/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-30] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-240\n",
            "Configuration saved in ./tmp/checkpoint-240/config.json\n",
            "Model weights saved in ./tmp/checkpoint-240/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-240/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-240/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-40] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-250\n",
            "Configuration saved in ./tmp/checkpoint-250/config.json\n",
            "Model weights saved in ./tmp/checkpoint-250/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-250/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-250/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-50] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-260\n",
            "Configuration saved in ./tmp/checkpoint-260/config.json\n",
            "Model weights saved in ./tmp/checkpoint-260/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-260/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-260/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-60] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-270\n",
            "Configuration saved in ./tmp/checkpoint-270/config.json\n",
            "Model weights saved in ./tmp/checkpoint-270/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-270/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-270/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-70] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-280\n",
            "Configuration saved in ./tmp/checkpoint-280/config.json\n",
            "Model weights saved in ./tmp/checkpoint-280/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-280/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-280/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-80] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-290\n",
            "Configuration saved in ./tmp/checkpoint-290/config.json\n",
            "Model weights saved in ./tmp/checkpoint-290/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-290/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-290/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-90] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-300\n",
            "Configuration saved in ./tmp/checkpoint-300/config.json\n",
            "Model weights saved in ./tmp/checkpoint-300/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-300/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-300/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-100] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-310\n",
            "Configuration saved in ./tmp/checkpoint-310/config.json\n",
            "Model weights saved in ./tmp/checkpoint-310/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-310/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-310/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-110] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-320\n",
            "Configuration saved in ./tmp/checkpoint-320/config.json\n",
            "Model weights saved in ./tmp/checkpoint-320/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-320/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-320/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-120] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-330\n",
            "Configuration saved in ./tmp/checkpoint-330/config.json\n",
            "Model weights saved in ./tmp/checkpoint-330/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-330/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-330/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-130] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-340\n",
            "Configuration saved in ./tmp/checkpoint-340/config.json\n",
            "Model weights saved in ./tmp/checkpoint-340/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-340/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-340/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-140] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-350\n",
            "Configuration saved in ./tmp/checkpoint-350/config.json\n",
            "Model weights saved in ./tmp/checkpoint-350/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-350/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-350/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-150] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-360\n",
            "Configuration saved in ./tmp/checkpoint-360/config.json\n",
            "Model weights saved in ./tmp/checkpoint-360/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-360/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-360/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-160] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-370\n",
            "Configuration saved in ./tmp/checkpoint-370/config.json\n",
            "Model weights saved in ./tmp/checkpoint-370/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-370/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-370/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-170] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-380\n",
            "Configuration saved in ./tmp/checkpoint-380/config.json\n",
            "Model weights saved in ./tmp/checkpoint-380/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-380/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-380/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-180] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-390\n",
            "Configuration saved in ./tmp/checkpoint-390/config.json\n",
            "Model weights saved in ./tmp/checkpoint-390/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-390/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-390/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-190] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-400\n",
            "Configuration saved in ./tmp/checkpoint-400/config.json\n",
            "Model weights saved in ./tmp/checkpoint-400/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-400/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-400/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-200] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-410\n",
            "Configuration saved in ./tmp/checkpoint-410/config.json\n",
            "Model weights saved in ./tmp/checkpoint-410/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-410/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-410/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-210] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-420\n",
            "Configuration saved in ./tmp/checkpoint-420/config.json\n",
            "Model weights saved in ./tmp/checkpoint-420/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-420/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-420/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-220] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-430\n",
            "Configuration saved in ./tmp/checkpoint-430/config.json\n",
            "Model weights saved in ./tmp/checkpoint-430/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-430/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-430/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-230] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-440\n",
            "Configuration saved in ./tmp/checkpoint-440/config.json\n",
            "Model weights saved in ./tmp/checkpoint-440/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-440/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-440/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-240] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-450\n",
            "Configuration saved in ./tmp/checkpoint-450/config.json\n",
            "Model weights saved in ./tmp/checkpoint-450/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-450/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-450/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-250] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-460\n",
            "Configuration saved in ./tmp/checkpoint-460/config.json\n",
            "Model weights saved in ./tmp/checkpoint-460/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-460/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-460/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-260] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-470\n",
            "Configuration saved in ./tmp/checkpoint-470/config.json\n",
            "Model weights saved in ./tmp/checkpoint-470/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-470/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-470/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-270] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-480\n",
            "Configuration saved in ./tmp/checkpoint-480/config.json\n",
            "Model weights saved in ./tmp/checkpoint-480/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-480/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-480/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-280] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-490\n",
            "Configuration saved in ./tmp/checkpoint-490/config.json\n",
            "Model weights saved in ./tmp/checkpoint-490/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-490/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-490/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-290] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-500\n",
            "Configuration saved in ./tmp/checkpoint-500/config.json\n",
            "Model weights saved in ./tmp/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-500/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-300] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-510\n",
            "Configuration saved in ./tmp/checkpoint-510/config.json\n",
            "Model weights saved in ./tmp/checkpoint-510/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-510/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-510/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-320] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-520\n",
            "Configuration saved in ./tmp/checkpoint-520/config.json\n",
            "Model weights saved in ./tmp/checkpoint-520/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-520/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-520/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-330] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-530\n",
            "Configuration saved in ./tmp/checkpoint-530/config.json\n",
            "Model weights saved in ./tmp/checkpoint-530/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-530/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-530/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-340] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-540\n",
            "Configuration saved in ./tmp/checkpoint-540/config.json\n",
            "Model weights saved in ./tmp/checkpoint-540/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-540/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-540/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-350] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-550\n",
            "Configuration saved in ./tmp/checkpoint-550/config.json\n",
            "Model weights saved in ./tmp/checkpoint-550/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-550/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-550/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-360] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-560\n",
            "Configuration saved in ./tmp/checkpoint-560/config.json\n",
            "Model weights saved in ./tmp/checkpoint-560/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-560/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-560/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-370] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-570\n",
            "Configuration saved in ./tmp/checkpoint-570/config.json\n",
            "Model weights saved in ./tmp/checkpoint-570/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-570/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-570/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-380] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-580\n",
            "Configuration saved in ./tmp/checkpoint-580/config.json\n",
            "Model weights saved in ./tmp/checkpoint-580/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-580/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-580/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-390] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-590\n",
            "Configuration saved in ./tmp/checkpoint-590/config.json\n",
            "Model weights saved in ./tmp/checkpoint-590/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-590/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-590/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-400] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./tmp/checkpoint-310 (score: 0.10352712869644165).\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: t1, t3, t2.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "####################################################################################################\n",
            "FOLD 0 SCORE: 0.3217563331127167\n",
            "####################################################################################################\n",
            "####################################################################################################\n",
            "TRAINING FOLD 1\n",
            "####################################################################################################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48a946c4d1c7416d8911a43a5cf214f2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cafb627ffd95461d94fd5576eb335f09",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "loading configuration file https://huggingface.co/allegro/herbert-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e512a74b9a525c868c0560254b3a65dc6309948f8cb99a8375429de673d28332.60289bb41d6aec457a093f9438a2c590beb41d64e7c830eac00dc56d3d49c5f6\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/allegro/herbert-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ca58839b8e4b1222703e13158ffeb3a5a7330260cbc39513f74710674d70268b.ad71128a5739887a02bfa6de2fa8768f86e02cd13d0c308873f4cdba254e4c7c\n",
            "Some weights of the model checkpoint at allegro/herbert-large-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.sso.sso_relationship.weight', 'cls.sso.sso_relationship.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "loading configuration file https://huggingface.co/allegro/herbert-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e512a74b9a525c868c0560254b3a65dc6309948f8cb99a8375429de673d28332.60289bb41d6aec457a093f9438a2c590beb41d64e7c830eac00dc56d3d49c5f6\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/allegro/herbert-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ca58839b8e4b1222703e13158ffeb3a5a7330260cbc39513f74710674d70268b.ad71128a5739887a02bfa6de2fa8768f86e02cd13d0c308873f4cdba254e4c7c\n",
            "Some weights of the model checkpoint at allegro/herbert-large-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.sso.sso_relationship.weight', 'cls.sso.sso_relationship.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running training *****\n",
            "  Num examples = 788\n",
            "  Num Epochs = 6\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 594\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='594' max='594' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [594/594 52:05, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.191112</td>\n",
              "      <td>0.437164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.142839</td>\n",
              "      <td>0.377940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.135059</td>\n",
              "      <td>0.367504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.157047</td>\n",
              "      <td>0.396292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.147140</td>\n",
              "      <td>0.383589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.152756</td>\n",
              "      <td>0.390840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.131573</td>\n",
              "      <td>0.362731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.103435</td>\n",
              "      <td>0.321613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.110469</td>\n",
              "      <td>0.332368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.114455</td>\n",
              "      <td>0.338313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.123677</td>\n",
              "      <td>0.351677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.134825</td>\n",
              "      <td>0.367185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.123168</td>\n",
              "      <td>0.350952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.106641</td>\n",
              "      <td>0.326559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.100679</td>\n",
              "      <td>0.317300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.122729</td>\n",
              "      <td>0.350327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.166789</td>\n",
              "      <td>0.408398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.111251</td>\n",
              "      <td>0.333543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.100677</td>\n",
              "      <td>0.317297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.116101</td>\n",
              "      <td>0.340736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.108774</td>\n",
              "      <td>0.329809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.137112</td>\n",
              "      <td>0.370286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.238921</td>\n",
              "      <td>0.488796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.227703</td>\n",
              "      <td>0.477182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.124727</td>\n",
              "      <td>0.353167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.136062</td>\n",
              "      <td>0.368865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.117902</td>\n",
              "      <td>0.343368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.108772</td>\n",
              "      <td>0.329806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.107286</td>\n",
              "      <td>0.327545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.101935</td>\n",
              "      <td>0.319273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.117329</td>\n",
              "      <td>0.342534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.131133</td>\n",
              "      <td>0.362123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.116987</td>\n",
              "      <td>0.342034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.174018</td>\n",
              "      <td>0.417154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.129948</td>\n",
              "      <td>0.360483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.102759</td>\n",
              "      <td>0.320561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.108279</td>\n",
              "      <td>0.329057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.116653</td>\n",
              "      <td>0.341545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.108199</td>\n",
              "      <td>0.328935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.105714</td>\n",
              "      <td>0.325137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.107188</td>\n",
              "      <td>0.327396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.111983</td>\n",
              "      <td>0.334638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.097211</td>\n",
              "      <td>0.311788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.115817</td>\n",
              "      <td>0.340319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.106445</td>\n",
              "      <td>0.326259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.099924</td>\n",
              "      <td>0.316108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.101411</td>\n",
              "      <td>0.318451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.117644</td>\n",
              "      <td>0.342992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.132215</td>\n",
              "      <td>0.363613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.149200</td>\n",
              "      <td>0.109325</td>\n",
              "      <td>0.330644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.149200</td>\n",
              "      <td>0.104547</td>\n",
              "      <td>0.323337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.149200</td>\n",
              "      <td>0.099360</td>\n",
              "      <td>0.315214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.149200</td>\n",
              "      <td>0.098248</td>\n",
              "      <td>0.313445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.149200</td>\n",
              "      <td>0.099147</td>\n",
              "      <td>0.314877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.149200</td>\n",
              "      <td>0.103477</td>\n",
              "      <td>0.321679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.149200</td>\n",
              "      <td>0.094786</td>\n",
              "      <td>0.307873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.149200</td>\n",
              "      <td>0.095610</td>\n",
              "      <td>0.309208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.149200</td>\n",
              "      <td>0.100617</td>\n",
              "      <td>0.317202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.149200</td>\n",
              "      <td>0.104761</td>\n",
              "      <td>0.323668</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-10\n",
            "Configuration saved in ./tmp/checkpoint-10/config.json\n",
            "Model weights saved in ./tmp/checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-10/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-310] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-20\n",
            "Configuration saved in ./tmp/checkpoint-20/config.json\n",
            "Model weights saved in ./tmp/checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-20/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-410] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-30\n",
            "Configuration saved in ./tmp/checkpoint-30/config.json\n",
            "Model weights saved in ./tmp/checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-30/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-420] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-40\n",
            "Configuration saved in ./tmp/checkpoint-40/config.json\n",
            "Model weights saved in ./tmp/checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-40/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-430] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-50\n",
            "Configuration saved in ./tmp/checkpoint-50/config.json\n",
            "Model weights saved in ./tmp/checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-50/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-440] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-60\n",
            "Configuration saved in ./tmp/checkpoint-60/config.json\n",
            "Model weights saved in ./tmp/checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-60/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-450] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-70\n",
            "Configuration saved in ./tmp/checkpoint-70/config.json\n",
            "Model weights saved in ./tmp/checkpoint-70/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-70/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-70/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-460] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-80\n",
            "Configuration saved in ./tmp/checkpoint-80/config.json\n",
            "Model weights saved in ./tmp/checkpoint-80/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-80/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-80/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-470] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-90\n",
            "Configuration saved in ./tmp/checkpoint-90/config.json\n",
            "Model weights saved in ./tmp/checkpoint-90/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-90/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-90/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-480] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-100\n",
            "Configuration saved in ./tmp/checkpoint-100/config.json\n",
            "Model weights saved in ./tmp/checkpoint-100/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-100/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-490] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-110\n",
            "Configuration saved in ./tmp/checkpoint-110/config.json\n",
            "Model weights saved in ./tmp/checkpoint-110/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-110/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-110/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-500] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-120\n",
            "Configuration saved in ./tmp/checkpoint-120/config.json\n",
            "Model weights saved in ./tmp/checkpoint-120/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-120/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-120/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-510] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-130\n",
            "Configuration saved in ./tmp/checkpoint-130/config.json\n",
            "Model weights saved in ./tmp/checkpoint-130/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-130/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-130/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-520] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-140\n",
            "Configuration saved in ./tmp/checkpoint-140/config.json\n",
            "Model weights saved in ./tmp/checkpoint-140/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-140/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-140/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-530] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-150\n",
            "Configuration saved in ./tmp/checkpoint-150/config.json\n",
            "Model weights saved in ./tmp/checkpoint-150/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-150/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-150/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-540] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-160\n",
            "Configuration saved in ./tmp/checkpoint-160/config.json\n",
            "Model weights saved in ./tmp/checkpoint-160/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-160/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-160/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-550] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-170\n",
            "Configuration saved in ./tmp/checkpoint-170/config.json\n",
            "Model weights saved in ./tmp/checkpoint-170/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-170/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-170/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-560] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-180\n",
            "Configuration saved in ./tmp/checkpoint-180/config.json\n",
            "Model weights saved in ./tmp/checkpoint-180/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-180/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-180/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-570] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-190\n",
            "Configuration saved in ./tmp/checkpoint-190/config.json\n",
            "Model weights saved in ./tmp/checkpoint-190/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-190/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-190/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-580] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-200\n",
            "Configuration saved in ./tmp/checkpoint-200/config.json\n",
            "Model weights saved in ./tmp/checkpoint-200/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-200/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-200/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-590] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-210\n",
            "Configuration saved in ./tmp/checkpoint-210/config.json\n",
            "Model weights saved in ./tmp/checkpoint-210/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-210/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-210/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-10] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-220\n",
            "Configuration saved in ./tmp/checkpoint-220/config.json\n",
            "Model weights saved in ./tmp/checkpoint-220/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-220/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-220/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-20] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-230\n",
            "Configuration saved in ./tmp/checkpoint-230/config.json\n",
            "Model weights saved in ./tmp/checkpoint-230/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-230/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-230/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-30] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-240\n",
            "Configuration saved in ./tmp/checkpoint-240/config.json\n",
            "Model weights saved in ./tmp/checkpoint-240/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-240/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-240/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-40] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-250\n",
            "Configuration saved in ./tmp/checkpoint-250/config.json\n",
            "Model weights saved in ./tmp/checkpoint-250/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-250/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-250/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-50] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-260\n",
            "Configuration saved in ./tmp/checkpoint-260/config.json\n",
            "Model weights saved in ./tmp/checkpoint-260/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-260/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-260/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-60] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-270\n",
            "Configuration saved in ./tmp/checkpoint-270/config.json\n",
            "Model weights saved in ./tmp/checkpoint-270/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-270/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-270/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-70] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-280\n",
            "Configuration saved in ./tmp/checkpoint-280/config.json\n",
            "Model weights saved in ./tmp/checkpoint-280/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-280/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-280/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-80] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-290\n",
            "Configuration saved in ./tmp/checkpoint-290/config.json\n",
            "Model weights saved in ./tmp/checkpoint-290/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-290/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-290/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-90] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-300\n",
            "Configuration saved in ./tmp/checkpoint-300/config.json\n",
            "Model weights saved in ./tmp/checkpoint-300/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-300/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-300/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-100] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-310\n",
            "Configuration saved in ./tmp/checkpoint-310/config.json\n",
            "Model weights saved in ./tmp/checkpoint-310/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-310/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-310/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-110] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-320\n",
            "Configuration saved in ./tmp/checkpoint-320/config.json\n",
            "Model weights saved in ./tmp/checkpoint-320/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-320/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-320/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-120] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-330\n",
            "Configuration saved in ./tmp/checkpoint-330/config.json\n",
            "Model weights saved in ./tmp/checkpoint-330/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-330/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-330/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-130] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-340\n",
            "Configuration saved in ./tmp/checkpoint-340/config.json\n",
            "Model weights saved in ./tmp/checkpoint-340/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-340/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-340/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-140] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-350\n",
            "Configuration saved in ./tmp/checkpoint-350/config.json\n",
            "Model weights saved in ./tmp/checkpoint-350/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-350/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-350/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-150] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-360\n",
            "Configuration saved in ./tmp/checkpoint-360/config.json\n",
            "Model weights saved in ./tmp/checkpoint-360/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-360/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-360/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-160] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-370\n",
            "Configuration saved in ./tmp/checkpoint-370/config.json\n",
            "Model weights saved in ./tmp/checkpoint-370/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-370/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-370/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-170] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-380\n",
            "Configuration saved in ./tmp/checkpoint-380/config.json\n",
            "Model weights saved in ./tmp/checkpoint-380/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-380/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-380/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-180] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-390\n",
            "Configuration saved in ./tmp/checkpoint-390/config.json\n",
            "Model weights saved in ./tmp/checkpoint-390/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-390/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-390/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-200] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-400\n",
            "Configuration saved in ./tmp/checkpoint-400/config.json\n",
            "Model weights saved in ./tmp/checkpoint-400/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-400/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-400/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-210] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-410\n",
            "Configuration saved in ./tmp/checkpoint-410/config.json\n",
            "Model weights saved in ./tmp/checkpoint-410/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-410/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-410/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-220] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-420\n",
            "Configuration saved in ./tmp/checkpoint-420/config.json\n",
            "Model weights saved in ./tmp/checkpoint-420/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-420/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-420/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-230] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-430\n",
            "Configuration saved in ./tmp/checkpoint-430/config.json\n",
            "Model weights saved in ./tmp/checkpoint-430/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-430/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-430/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-190] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-440\n",
            "Configuration saved in ./tmp/checkpoint-440/config.json\n",
            "Model weights saved in ./tmp/checkpoint-440/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-440/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-440/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-240] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-450\n",
            "Configuration saved in ./tmp/checkpoint-450/config.json\n",
            "Model weights saved in ./tmp/checkpoint-450/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-450/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-450/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-250] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-460\n",
            "Configuration saved in ./tmp/checkpoint-460/config.json\n",
            "Model weights saved in ./tmp/checkpoint-460/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-460/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-460/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-260] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-470\n",
            "Configuration saved in ./tmp/checkpoint-470/config.json\n",
            "Model weights saved in ./tmp/checkpoint-470/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-470/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-470/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-270] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-480\n",
            "Configuration saved in ./tmp/checkpoint-480/config.json\n",
            "Model weights saved in ./tmp/checkpoint-480/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-480/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-480/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-280] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-490\n",
            "Configuration saved in ./tmp/checkpoint-490/config.json\n",
            "Model weights saved in ./tmp/checkpoint-490/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-490/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-490/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-290] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-500\n",
            "Configuration saved in ./tmp/checkpoint-500/config.json\n",
            "Model weights saved in ./tmp/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-500/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-300] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-510\n",
            "Configuration saved in ./tmp/checkpoint-510/config.json\n",
            "Model weights saved in ./tmp/checkpoint-510/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-510/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-510/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-310] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-520\n",
            "Configuration saved in ./tmp/checkpoint-520/config.json\n",
            "Model weights saved in ./tmp/checkpoint-520/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-520/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-520/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-320] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-530\n",
            "Configuration saved in ./tmp/checkpoint-530/config.json\n",
            "Model weights saved in ./tmp/checkpoint-530/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-530/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-530/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-330] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-540\n",
            "Configuration saved in ./tmp/checkpoint-540/config.json\n",
            "Model weights saved in ./tmp/checkpoint-540/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-540/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-540/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-340] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-550\n",
            "Configuration saved in ./tmp/checkpoint-550/config.json\n",
            "Model weights saved in ./tmp/checkpoint-550/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-550/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-550/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-350] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-560\n",
            "Configuration saved in ./tmp/checkpoint-560/config.json\n",
            "Model weights saved in ./tmp/checkpoint-560/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-560/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-560/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-360] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-570\n",
            "Configuration saved in ./tmp/checkpoint-570/config.json\n",
            "Model weights saved in ./tmp/checkpoint-570/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-570/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-570/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-370] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-580\n",
            "Configuration saved in ./tmp/checkpoint-580/config.json\n",
            "Model weights saved in ./tmp/checkpoint-580/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-580/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-580/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-380] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-590\n",
            "Configuration saved in ./tmp/checkpoint-590/config.json\n",
            "Model weights saved in ./tmp/checkpoint-590/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-590/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-590/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-390] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./tmp/checkpoint-560 (score: 0.09478587657213211).\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: t1, t3, t2.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "####################################################################################################\n",
            "FOLD 1 SCORE: 0.30787310004234314\n",
            "####################################################################################################\n",
            "####################################################################################################\n",
            "TRAINING FOLD 2\n",
            "####################################################################################################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "022e904b4265497bbe41d22a5f23a12b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f58f380f8244147a296dd2b973d540c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "loading configuration file https://huggingface.co/allegro/herbert-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e512a74b9a525c868c0560254b3a65dc6309948f8cb99a8375429de673d28332.60289bb41d6aec457a093f9438a2c590beb41d64e7c830eac00dc56d3d49c5f6\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/allegro/herbert-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ca58839b8e4b1222703e13158ffeb3a5a7330260cbc39513f74710674d70268b.ad71128a5739887a02bfa6de2fa8768f86e02cd13d0c308873f4cdba254e4c7c\n",
            "Some weights of the model checkpoint at allegro/herbert-large-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.sso.sso_relationship.weight', 'cls.sso.sso_relationship.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "loading configuration file https://huggingface.co/allegro/herbert-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e512a74b9a525c868c0560254b3a65dc6309948f8cb99a8375429de673d28332.60289bb41d6aec457a093f9438a2c590beb41d64e7c830eac00dc56d3d49c5f6\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/allegro/herbert-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ca58839b8e4b1222703e13158ffeb3a5a7330260cbc39513f74710674d70268b.ad71128a5739887a02bfa6de2fa8768f86e02cd13d0c308873f4cdba254e4c7c\n",
            "Some weights of the model checkpoint at allegro/herbert-large-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.sso.sso_relationship.weight', 'cls.sso.sso_relationship.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running training *****\n",
            "  Num examples = 788\n",
            "  Num Epochs = 6\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 594\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='594' max='594' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [594/594 52:03, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.193161</td>\n",
              "      <td>0.439501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.151303</td>\n",
              "      <td>0.388977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.130335</td>\n",
              "      <td>0.361019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.112154</td>\n",
              "      <td>0.334895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.103676</td>\n",
              "      <td>0.321988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.107661</td>\n",
              "      <td>0.328118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.098992</td>\n",
              "      <td>0.314630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.095458</td>\n",
              "      <td>0.308962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.092479</td>\n",
              "      <td>0.304104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.091311</td>\n",
              "      <td>0.302178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.228642</td>\n",
              "      <td>0.478166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.094858</td>\n",
              "      <td>0.307990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.092739</td>\n",
              "      <td>0.304530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.092837</td>\n",
              "      <td>0.304691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.101106</td>\n",
              "      <td>0.317972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.125682</td>\n",
              "      <td>0.354516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.096376</td>\n",
              "      <td>0.310444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.087196</td>\n",
              "      <td>0.295289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.127670</td>\n",
              "      <td>0.357310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.087759</td>\n",
              "      <td>0.296241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.110078</td>\n",
              "      <td>0.331780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.100693</td>\n",
              "      <td>0.317322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.240210</td>\n",
              "      <td>0.490113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.185060</td>\n",
              "      <td>0.430185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.092899</td>\n",
              "      <td>0.304794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.088993</td>\n",
              "      <td>0.298317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.094939</td>\n",
              "      <td>0.308122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.086742</td>\n",
              "      <td>0.294521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.095350</td>\n",
              "      <td>0.308789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.089252</td>\n",
              "      <td>0.298750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.108880</td>\n",
              "      <td>0.329969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.126552</td>\n",
              "      <td>0.355742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.123362</td>\n",
              "      <td>0.351229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.089168</td>\n",
              "      <td>0.298610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.090695</td>\n",
              "      <td>0.301156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.144483</td>\n",
              "      <td>0.380109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.087290</td>\n",
              "      <td>0.295449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.103567</td>\n",
              "      <td>0.321819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.092859</td>\n",
              "      <td>0.304728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.084940</td>\n",
              "      <td>0.291444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.085908</td>\n",
              "      <td>0.293101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.092088</td>\n",
              "      <td>0.303459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.082150</td>\n",
              "      <td>0.286618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.089430</td>\n",
              "      <td>0.299048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.084267</td>\n",
              "      <td>0.290288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.084324</td>\n",
              "      <td>0.290385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.144152</td>\n",
              "      <td>0.379673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.087758</td>\n",
              "      <td>0.296240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.084247</td>\n",
              "      <td>0.290253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.116132</td>\n",
              "      <td>0.340781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.091586</td>\n",
              "      <td>0.302633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.081462</td>\n",
              "      <td>0.285415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.083100</td>\n",
              "      <td>0.288271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.080317</td>\n",
              "      <td>0.283403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.083375</td>\n",
              "      <td>0.288748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.098532</td>\n",
              "      <td>0.313898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.085069</td>\n",
              "      <td>0.291666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.119053</td>\n",
              "      <td>0.345040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.089474</td>\n",
              "      <td>0.299122</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-10\n",
            "Configuration saved in ./tmp/checkpoint-10/config.json\n",
            "Model weights saved in ./tmp/checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-10/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-400] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-20\n",
            "Configuration saved in ./tmp/checkpoint-20/config.json\n",
            "Model weights saved in ./tmp/checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-20/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-410] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-30\n",
            "Configuration saved in ./tmp/checkpoint-30/config.json\n",
            "Model weights saved in ./tmp/checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-30/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-420] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-40\n",
            "Configuration saved in ./tmp/checkpoint-40/config.json\n",
            "Model weights saved in ./tmp/checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-40/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-430] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-50\n",
            "Configuration saved in ./tmp/checkpoint-50/config.json\n",
            "Model weights saved in ./tmp/checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-50/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-440] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-60\n",
            "Configuration saved in ./tmp/checkpoint-60/config.json\n",
            "Model weights saved in ./tmp/checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-60/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-450] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-70\n",
            "Configuration saved in ./tmp/checkpoint-70/config.json\n",
            "Model weights saved in ./tmp/checkpoint-70/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-70/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-70/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-460] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-80\n",
            "Configuration saved in ./tmp/checkpoint-80/config.json\n",
            "Model weights saved in ./tmp/checkpoint-80/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-80/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-80/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-470] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-90\n",
            "Configuration saved in ./tmp/checkpoint-90/config.json\n",
            "Model weights saved in ./tmp/checkpoint-90/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-90/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-90/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-480] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-100\n",
            "Configuration saved in ./tmp/checkpoint-100/config.json\n",
            "Model weights saved in ./tmp/checkpoint-100/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-100/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-490] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-110\n",
            "Configuration saved in ./tmp/checkpoint-110/config.json\n",
            "Model weights saved in ./tmp/checkpoint-110/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-110/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-110/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-500] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-120\n",
            "Configuration saved in ./tmp/checkpoint-120/config.json\n",
            "Model weights saved in ./tmp/checkpoint-120/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-120/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-120/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-510] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-130\n",
            "Configuration saved in ./tmp/checkpoint-130/config.json\n",
            "Model weights saved in ./tmp/checkpoint-130/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-130/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-130/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-520] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-140\n",
            "Configuration saved in ./tmp/checkpoint-140/config.json\n",
            "Model weights saved in ./tmp/checkpoint-140/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-140/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-140/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-530] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-150\n",
            "Configuration saved in ./tmp/checkpoint-150/config.json\n",
            "Model weights saved in ./tmp/checkpoint-150/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-150/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-150/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-540] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-160\n",
            "Configuration saved in ./tmp/checkpoint-160/config.json\n",
            "Model weights saved in ./tmp/checkpoint-160/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-160/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-160/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-550] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-170\n",
            "Configuration saved in ./tmp/checkpoint-170/config.json\n",
            "Model weights saved in ./tmp/checkpoint-170/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-170/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-170/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-560] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-180\n",
            "Configuration saved in ./tmp/checkpoint-180/config.json\n",
            "Model weights saved in ./tmp/checkpoint-180/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-180/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-180/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-570] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-190\n",
            "Configuration saved in ./tmp/checkpoint-190/config.json\n",
            "Model weights saved in ./tmp/checkpoint-190/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-190/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-190/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-580] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-200\n",
            "Configuration saved in ./tmp/checkpoint-200/config.json\n",
            "Model weights saved in ./tmp/checkpoint-200/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-200/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-200/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-590] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-210\n",
            "Configuration saved in ./tmp/checkpoint-210/config.json\n",
            "Model weights saved in ./tmp/checkpoint-210/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-210/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-210/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-10] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-220\n",
            "Configuration saved in ./tmp/checkpoint-220/config.json\n",
            "Model weights saved in ./tmp/checkpoint-220/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-220/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-220/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-20] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-230\n",
            "Configuration saved in ./tmp/checkpoint-230/config.json\n",
            "Model weights saved in ./tmp/checkpoint-230/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-230/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-230/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-30] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-240\n",
            "Configuration saved in ./tmp/checkpoint-240/config.json\n",
            "Model weights saved in ./tmp/checkpoint-240/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-240/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-240/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-40] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-250\n",
            "Configuration saved in ./tmp/checkpoint-250/config.json\n",
            "Model weights saved in ./tmp/checkpoint-250/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-250/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-250/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-50] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-260\n",
            "Configuration saved in ./tmp/checkpoint-260/config.json\n",
            "Model weights saved in ./tmp/checkpoint-260/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-260/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-260/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-60] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-270\n",
            "Configuration saved in ./tmp/checkpoint-270/config.json\n",
            "Model weights saved in ./tmp/checkpoint-270/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-270/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-270/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-70] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-280\n",
            "Configuration saved in ./tmp/checkpoint-280/config.json\n",
            "Model weights saved in ./tmp/checkpoint-280/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-280/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-280/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-80] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-290\n",
            "Configuration saved in ./tmp/checkpoint-290/config.json\n",
            "Model weights saved in ./tmp/checkpoint-290/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-290/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-290/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-90] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-300\n",
            "Configuration saved in ./tmp/checkpoint-300/config.json\n",
            "Model weights saved in ./tmp/checkpoint-300/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-300/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-300/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-100] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-310\n",
            "Configuration saved in ./tmp/checkpoint-310/config.json\n",
            "Model weights saved in ./tmp/checkpoint-310/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-310/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-310/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-110] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-320\n",
            "Configuration saved in ./tmp/checkpoint-320/config.json\n",
            "Model weights saved in ./tmp/checkpoint-320/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-320/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-320/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-120] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-330\n",
            "Configuration saved in ./tmp/checkpoint-330/config.json\n",
            "Model weights saved in ./tmp/checkpoint-330/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-330/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-330/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-130] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-340\n",
            "Configuration saved in ./tmp/checkpoint-340/config.json\n",
            "Model weights saved in ./tmp/checkpoint-340/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-340/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-340/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-140] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-350\n",
            "Configuration saved in ./tmp/checkpoint-350/config.json\n",
            "Model weights saved in ./tmp/checkpoint-350/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-350/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-350/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-150] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-360\n",
            "Configuration saved in ./tmp/checkpoint-360/config.json\n",
            "Model weights saved in ./tmp/checkpoint-360/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-360/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-360/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-160] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-370\n",
            "Configuration saved in ./tmp/checkpoint-370/config.json\n",
            "Model weights saved in ./tmp/checkpoint-370/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-370/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-370/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-170] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-380\n",
            "Configuration saved in ./tmp/checkpoint-380/config.json\n",
            "Model weights saved in ./tmp/checkpoint-380/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-380/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-380/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-180] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-390\n",
            "Configuration saved in ./tmp/checkpoint-390/config.json\n",
            "Model weights saved in ./tmp/checkpoint-390/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-390/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-390/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-190] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-400\n",
            "Configuration saved in ./tmp/checkpoint-400/config.json\n",
            "Model weights saved in ./tmp/checkpoint-400/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-400/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-400/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-200] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-410\n",
            "Configuration saved in ./tmp/checkpoint-410/config.json\n",
            "Model weights saved in ./tmp/checkpoint-410/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-410/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-410/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-210] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-420\n",
            "Configuration saved in ./tmp/checkpoint-420/config.json\n",
            "Model weights saved in ./tmp/checkpoint-420/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-420/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-420/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-220] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-430\n",
            "Configuration saved in ./tmp/checkpoint-430/config.json\n",
            "Model weights saved in ./tmp/checkpoint-430/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-430/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-430/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-230] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-440\n",
            "Configuration saved in ./tmp/checkpoint-440/config.json\n",
            "Model weights saved in ./tmp/checkpoint-440/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-440/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-440/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-240] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-450\n",
            "Configuration saved in ./tmp/checkpoint-450/config.json\n",
            "Model weights saved in ./tmp/checkpoint-450/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-450/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-450/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-250] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-460\n",
            "Configuration saved in ./tmp/checkpoint-460/config.json\n",
            "Model weights saved in ./tmp/checkpoint-460/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-460/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-460/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-260] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-470\n",
            "Configuration saved in ./tmp/checkpoint-470/config.json\n",
            "Model weights saved in ./tmp/checkpoint-470/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-470/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-470/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-270] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-480\n",
            "Configuration saved in ./tmp/checkpoint-480/config.json\n",
            "Model weights saved in ./tmp/checkpoint-480/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-480/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-480/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-280] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-490\n",
            "Configuration saved in ./tmp/checkpoint-490/config.json\n",
            "Model weights saved in ./tmp/checkpoint-490/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-490/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-490/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-290] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-500\n",
            "Configuration saved in ./tmp/checkpoint-500/config.json\n",
            "Model weights saved in ./tmp/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-500/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-300] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-510\n",
            "Configuration saved in ./tmp/checkpoint-510/config.json\n",
            "Model weights saved in ./tmp/checkpoint-510/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-510/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-510/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-310] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-520\n",
            "Configuration saved in ./tmp/checkpoint-520/config.json\n",
            "Model weights saved in ./tmp/checkpoint-520/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-520/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-520/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-320] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-530\n",
            "Configuration saved in ./tmp/checkpoint-530/config.json\n",
            "Model weights saved in ./tmp/checkpoint-530/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-530/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-530/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-330] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-540\n",
            "Configuration saved in ./tmp/checkpoint-540/config.json\n",
            "Model weights saved in ./tmp/checkpoint-540/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-540/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-540/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-340] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-550\n",
            "Configuration saved in ./tmp/checkpoint-550/config.json\n",
            "Model weights saved in ./tmp/checkpoint-550/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-550/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-550/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-350] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-560\n",
            "Configuration saved in ./tmp/checkpoint-560/config.json\n",
            "Model weights saved in ./tmp/checkpoint-560/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-560/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-560/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-360] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-570\n",
            "Configuration saved in ./tmp/checkpoint-570/config.json\n",
            "Model weights saved in ./tmp/checkpoint-570/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-570/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-570/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-370] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-580\n",
            "Configuration saved in ./tmp/checkpoint-580/config.json\n",
            "Model weights saved in ./tmp/checkpoint-580/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-580/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-580/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-380] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-590\n",
            "Configuration saved in ./tmp/checkpoint-590/config.json\n",
            "Model weights saved in ./tmp/checkpoint-590/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-590/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-590/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-390] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./tmp/checkpoint-540 (score: 0.08031709492206573).\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: t1, t3, t2.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "####################################################################################################\n",
            "FOLD 2 SCORE: 0.2834027111530304\n",
            "####################################################################################################\n",
            "####################################################################################################\n",
            "TRAINING FOLD 3\n",
            "####################################################################################################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "561877ff5d6444bc8643db7718ba8ed5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a63aab736a43421bb2653fe7a3ae3d0e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "loading configuration file https://huggingface.co/allegro/herbert-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e512a74b9a525c868c0560254b3a65dc6309948f8cb99a8375429de673d28332.60289bb41d6aec457a093f9438a2c590beb41d64e7c830eac00dc56d3d49c5f6\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/allegro/herbert-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ca58839b8e4b1222703e13158ffeb3a5a7330260cbc39513f74710674d70268b.ad71128a5739887a02bfa6de2fa8768f86e02cd13d0c308873f4cdba254e4c7c\n",
            "Some weights of the model checkpoint at allegro/herbert-large-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.sso.sso_relationship.weight', 'cls.sso.sso_relationship.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "loading configuration file https://huggingface.co/allegro/herbert-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e512a74b9a525c868c0560254b3a65dc6309948f8cb99a8375429de673d28332.60289bb41d6aec457a093f9438a2c590beb41d64e7c830eac00dc56d3d49c5f6\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/allegro/herbert-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ca58839b8e4b1222703e13158ffeb3a5a7330260cbc39513f74710674d70268b.ad71128a5739887a02bfa6de2fa8768f86e02cd13d0c308873f4cdba254e4c7c\n",
            "Some weights of the model checkpoint at allegro/herbert-large-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.sso.sso_relationship.weight', 'cls.sso.sso_relationship.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running training *****\n",
            "  Num examples = 788\n",
            "  Num Epochs = 6\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 594\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='594' max='594' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [594/594 52:03, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.222673</td>\n",
              "      <td>0.471882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.185826</td>\n",
              "      <td>0.431075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.144534</td>\n",
              "      <td>0.380176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.139801</td>\n",
              "      <td>0.373900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.218972</td>\n",
              "      <td>0.467944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.175998</td>\n",
              "      <td>0.419521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.128056</td>\n",
              "      <td>0.357850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.172502</td>\n",
              "      <td>0.415333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.275705</td>\n",
              "      <td>0.525077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.124108</td>\n",
              "      <td>0.352290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.113668</td>\n",
              "      <td>0.337147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.139231</td>\n",
              "      <td>0.373137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.108354</td>\n",
              "      <td>0.329172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.126228</td>\n",
              "      <td>0.355286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.115175</td>\n",
              "      <td>0.339375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.120206</td>\n",
              "      <td>0.346708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.134710</td>\n",
              "      <td>0.367029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.137914</td>\n",
              "      <td>0.371368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.121275</td>\n",
              "      <td>0.348245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.149835</td>\n",
              "      <td>0.387085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.106644</td>\n",
              "      <td>0.326564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.104204</td>\n",
              "      <td>0.322807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.103675</td>\n",
              "      <td>0.321987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.098632</td>\n",
              "      <td>0.314057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.100431</td>\n",
              "      <td>0.316908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.107989</td>\n",
              "      <td>0.328616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.110122</td>\n",
              "      <td>0.331847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.124045</td>\n",
              "      <td>0.352201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.109327</td>\n",
              "      <td>0.330647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.105994</td>\n",
              "      <td>0.325567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.142340</td>\n",
              "      <td>0.377280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.107607</td>\n",
              "      <td>0.328035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.116935</td>\n",
              "      <td>0.341958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.113185</td>\n",
              "      <td>0.336429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.116180</td>\n",
              "      <td>0.340851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.113842</td>\n",
              "      <td>0.337404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.109379</td>\n",
              "      <td>0.330724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.105350</td>\n",
              "      <td>0.324577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.128157</td>\n",
              "      <td>0.357991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.107767</td>\n",
              "      <td>0.328278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.112116</td>\n",
              "      <td>0.334838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.107770</td>\n",
              "      <td>0.328283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.110337</td>\n",
              "      <td>0.332170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.107854</td>\n",
              "      <td>0.328411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.117928</td>\n",
              "      <td>0.343406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.167967</td>\n",
              "      <td>0.409837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.141947</td>\n",
              "      <td>0.376758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.101889</td>\n",
              "      <td>0.319200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.126126</td>\n",
              "      <td>0.355142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.110235</td>\n",
              "      <td>0.332016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.108626</td>\n",
              "      <td>0.329585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.105001</td>\n",
              "      <td>0.324039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.103660</td>\n",
              "      <td>0.321963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.104552</td>\n",
              "      <td>0.323345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.101080</td>\n",
              "      <td>0.317930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.104484</td>\n",
              "      <td>0.323239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.102575</td>\n",
              "      <td>0.320274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.106017</td>\n",
              "      <td>0.325602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.108854</td>\n",
              "      <td>0.329930</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-10\n",
            "Configuration saved in ./tmp/checkpoint-10/config.json\n",
            "Model weights saved in ./tmp/checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-10/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-400] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-20\n",
            "Configuration saved in ./tmp/checkpoint-20/config.json\n",
            "Model weights saved in ./tmp/checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-20/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-410] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-30\n",
            "Configuration saved in ./tmp/checkpoint-30/config.json\n",
            "Model weights saved in ./tmp/checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-30/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-420] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-40\n",
            "Configuration saved in ./tmp/checkpoint-40/config.json\n",
            "Model weights saved in ./tmp/checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-40/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-430] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-50\n",
            "Configuration saved in ./tmp/checkpoint-50/config.json\n",
            "Model weights saved in ./tmp/checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-50/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-440] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-60\n",
            "Configuration saved in ./tmp/checkpoint-60/config.json\n",
            "Model weights saved in ./tmp/checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-60/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-450] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-70\n",
            "Configuration saved in ./tmp/checkpoint-70/config.json\n",
            "Model weights saved in ./tmp/checkpoint-70/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-70/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-70/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-460] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-80\n",
            "Configuration saved in ./tmp/checkpoint-80/config.json\n",
            "Model weights saved in ./tmp/checkpoint-80/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-80/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-80/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-470] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-90\n",
            "Configuration saved in ./tmp/checkpoint-90/config.json\n",
            "Model weights saved in ./tmp/checkpoint-90/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-90/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-90/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-480] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-100\n",
            "Configuration saved in ./tmp/checkpoint-100/config.json\n",
            "Model weights saved in ./tmp/checkpoint-100/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-100/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-490] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-110\n",
            "Configuration saved in ./tmp/checkpoint-110/config.json\n",
            "Model weights saved in ./tmp/checkpoint-110/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-110/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-110/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-500] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-120\n",
            "Configuration saved in ./tmp/checkpoint-120/config.json\n",
            "Model weights saved in ./tmp/checkpoint-120/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-120/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-120/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-510] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-130\n",
            "Configuration saved in ./tmp/checkpoint-130/config.json\n",
            "Model weights saved in ./tmp/checkpoint-130/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-130/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-130/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-520] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-140\n",
            "Configuration saved in ./tmp/checkpoint-140/config.json\n",
            "Model weights saved in ./tmp/checkpoint-140/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-140/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-140/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-530] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-150\n",
            "Configuration saved in ./tmp/checkpoint-150/config.json\n",
            "Model weights saved in ./tmp/checkpoint-150/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-150/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-150/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-540] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-160\n",
            "Configuration saved in ./tmp/checkpoint-160/config.json\n",
            "Model weights saved in ./tmp/checkpoint-160/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-160/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-160/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-550] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-170\n",
            "Configuration saved in ./tmp/checkpoint-170/config.json\n",
            "Model weights saved in ./tmp/checkpoint-170/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-170/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-170/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-560] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-180\n",
            "Configuration saved in ./tmp/checkpoint-180/config.json\n",
            "Model weights saved in ./tmp/checkpoint-180/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-180/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-180/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-570] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-190\n",
            "Configuration saved in ./tmp/checkpoint-190/config.json\n",
            "Model weights saved in ./tmp/checkpoint-190/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-190/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-190/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-580] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-200\n",
            "Configuration saved in ./tmp/checkpoint-200/config.json\n",
            "Model weights saved in ./tmp/checkpoint-200/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-200/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-200/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-590] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-210\n",
            "Configuration saved in ./tmp/checkpoint-210/config.json\n",
            "Model weights saved in ./tmp/checkpoint-210/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-210/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-210/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-10] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-220\n",
            "Configuration saved in ./tmp/checkpoint-220/config.json\n",
            "Model weights saved in ./tmp/checkpoint-220/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-220/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-220/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-20] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-230\n",
            "Configuration saved in ./tmp/checkpoint-230/config.json\n",
            "Model weights saved in ./tmp/checkpoint-230/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-230/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-230/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-30] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-240\n",
            "Configuration saved in ./tmp/checkpoint-240/config.json\n",
            "Model weights saved in ./tmp/checkpoint-240/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-240/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-240/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-40] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-250\n",
            "Configuration saved in ./tmp/checkpoint-250/config.json\n",
            "Model weights saved in ./tmp/checkpoint-250/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-250/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-250/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-50] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-260\n",
            "Configuration saved in ./tmp/checkpoint-260/config.json\n",
            "Model weights saved in ./tmp/checkpoint-260/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-260/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-260/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-60] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-270\n",
            "Configuration saved in ./tmp/checkpoint-270/config.json\n",
            "Model weights saved in ./tmp/checkpoint-270/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-270/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-270/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-70] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-280\n",
            "Configuration saved in ./tmp/checkpoint-280/config.json\n",
            "Model weights saved in ./tmp/checkpoint-280/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-280/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-280/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-80] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-290\n",
            "Configuration saved in ./tmp/checkpoint-290/config.json\n",
            "Model weights saved in ./tmp/checkpoint-290/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-290/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-290/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-90] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-300\n",
            "Configuration saved in ./tmp/checkpoint-300/config.json\n",
            "Model weights saved in ./tmp/checkpoint-300/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-300/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-300/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-100] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-310\n",
            "Configuration saved in ./tmp/checkpoint-310/config.json\n",
            "Model weights saved in ./tmp/checkpoint-310/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-310/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-310/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-110] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-320\n",
            "Configuration saved in ./tmp/checkpoint-320/config.json\n",
            "Model weights saved in ./tmp/checkpoint-320/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-320/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-320/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-120] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-330\n",
            "Configuration saved in ./tmp/checkpoint-330/config.json\n",
            "Model weights saved in ./tmp/checkpoint-330/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-330/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-330/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-130] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-340\n",
            "Configuration saved in ./tmp/checkpoint-340/config.json\n",
            "Model weights saved in ./tmp/checkpoint-340/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-340/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-340/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-140] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-350\n",
            "Configuration saved in ./tmp/checkpoint-350/config.json\n",
            "Model weights saved in ./tmp/checkpoint-350/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-350/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-350/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-150] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-360\n",
            "Configuration saved in ./tmp/checkpoint-360/config.json\n",
            "Model weights saved in ./tmp/checkpoint-360/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-360/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-360/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-160] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-370\n",
            "Configuration saved in ./tmp/checkpoint-370/config.json\n",
            "Model weights saved in ./tmp/checkpoint-370/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-370/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-370/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-170] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-380\n",
            "Configuration saved in ./tmp/checkpoint-380/config.json\n",
            "Model weights saved in ./tmp/checkpoint-380/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-380/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-380/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-180] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-390\n",
            "Configuration saved in ./tmp/checkpoint-390/config.json\n",
            "Model weights saved in ./tmp/checkpoint-390/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-390/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-390/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-190] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-400\n",
            "Configuration saved in ./tmp/checkpoint-400/config.json\n",
            "Model weights saved in ./tmp/checkpoint-400/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-400/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-400/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-200] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-410\n",
            "Configuration saved in ./tmp/checkpoint-410/config.json\n",
            "Model weights saved in ./tmp/checkpoint-410/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-410/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-410/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-210] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-420\n",
            "Configuration saved in ./tmp/checkpoint-420/config.json\n",
            "Model weights saved in ./tmp/checkpoint-420/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-420/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-420/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-220] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-430\n",
            "Configuration saved in ./tmp/checkpoint-430/config.json\n",
            "Model weights saved in ./tmp/checkpoint-430/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-430/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-430/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-230] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-440\n",
            "Configuration saved in ./tmp/checkpoint-440/config.json\n",
            "Model weights saved in ./tmp/checkpoint-440/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-440/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-440/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-250] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-450\n",
            "Configuration saved in ./tmp/checkpoint-450/config.json\n",
            "Model weights saved in ./tmp/checkpoint-450/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-450/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-450/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-260] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-460\n",
            "Configuration saved in ./tmp/checkpoint-460/config.json\n",
            "Model weights saved in ./tmp/checkpoint-460/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-460/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-460/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-270] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-470\n",
            "Configuration saved in ./tmp/checkpoint-470/config.json\n",
            "Model weights saved in ./tmp/checkpoint-470/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-470/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-470/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-280] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-480\n",
            "Configuration saved in ./tmp/checkpoint-480/config.json\n",
            "Model weights saved in ./tmp/checkpoint-480/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-480/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-480/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-290] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-490\n",
            "Configuration saved in ./tmp/checkpoint-490/config.json\n",
            "Model weights saved in ./tmp/checkpoint-490/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-490/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-490/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-300] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-500\n",
            "Configuration saved in ./tmp/checkpoint-500/config.json\n",
            "Model weights saved in ./tmp/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-500/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-310] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-510\n",
            "Configuration saved in ./tmp/checkpoint-510/config.json\n",
            "Model weights saved in ./tmp/checkpoint-510/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-510/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-510/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-320] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-520\n",
            "Configuration saved in ./tmp/checkpoint-520/config.json\n",
            "Model weights saved in ./tmp/checkpoint-520/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-520/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-520/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-330] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-530\n",
            "Configuration saved in ./tmp/checkpoint-530/config.json\n",
            "Model weights saved in ./tmp/checkpoint-530/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-530/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-530/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-340] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-540\n",
            "Configuration saved in ./tmp/checkpoint-540/config.json\n",
            "Model weights saved in ./tmp/checkpoint-540/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-540/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-540/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-350] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-550\n",
            "Configuration saved in ./tmp/checkpoint-550/config.json\n",
            "Model weights saved in ./tmp/checkpoint-550/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-550/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-550/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-360] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-560\n",
            "Configuration saved in ./tmp/checkpoint-560/config.json\n",
            "Model weights saved in ./tmp/checkpoint-560/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-560/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-560/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-370] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-570\n",
            "Configuration saved in ./tmp/checkpoint-570/config.json\n",
            "Model weights saved in ./tmp/checkpoint-570/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-570/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-570/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-380] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-580\n",
            "Configuration saved in ./tmp/checkpoint-580/config.json\n",
            "Model weights saved in ./tmp/checkpoint-580/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-580/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-580/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-390] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-590\n",
            "Configuration saved in ./tmp/checkpoint-590/config.json\n",
            "Model weights saved in ./tmp/checkpoint-590/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-590/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-590/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-400] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./tmp/checkpoint-240 (score: 0.09863180667161942).\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: t1, t3, t2.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "####################################################################################################\n",
            "FOLD 3 SCORE: 0.31405699253082275\n",
            "####################################################################################################\n",
            "####################################################################################################\n",
            "TRAINING FOLD 4\n",
            "####################################################################################################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "291e0568c6ad4d4d9d2cacbceb83efbe",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd731a30265142e3a4dd596873f4f78a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "loading configuration file https://huggingface.co/allegro/herbert-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e512a74b9a525c868c0560254b3a65dc6309948f8cb99a8375429de673d28332.60289bb41d6aec457a093f9438a2c590beb41d64e7c830eac00dc56d3d49c5f6\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/allegro/herbert-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ca58839b8e4b1222703e13158ffeb3a5a7330260cbc39513f74710674d70268b.ad71128a5739887a02bfa6de2fa8768f86e02cd13d0c308873f4cdba254e4c7c\n",
            "Some weights of the model checkpoint at allegro/herbert-large-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.sso.sso_relationship.weight', 'cls.sso.sso_relationship.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "loading configuration file https://huggingface.co/allegro/herbert-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e512a74b9a525c868c0560254b3a65dc6309948f8cb99a8375429de673d28332.60289bb41d6aec457a093f9438a2c590beb41d64e7c830eac00dc56d3d49c5f6\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/allegro/herbert-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ca58839b8e4b1222703e13158ffeb3a5a7330260cbc39513f74710674d70268b.ad71128a5739887a02bfa6de2fa8768f86e02cd13d0c308873f4cdba254e4c7c\n",
            "Some weights of the model checkpoint at allegro/herbert-large-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.sso.sso_relationship.weight', 'cls.sso.sso_relationship.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running training *****\n",
            "  Num examples = 788\n",
            "  Num Epochs = 6\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 594\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='594' max='594' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [594/594 52:03, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.252013</td>\n",
              "      <td>0.502008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.122973</td>\n",
              "      <td>0.350675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.175666</td>\n",
              "      <td>0.419126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.119155</td>\n",
              "      <td>0.345188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.327034</td>\n",
              "      <td>0.571869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.133206</td>\n",
              "      <td>0.364973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.117895</td>\n",
              "      <td>0.343359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.118186</td>\n",
              "      <td>0.343782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.110969</td>\n",
              "      <td>0.333120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.155181</td>\n",
              "      <td>0.393930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.156087</td>\n",
              "      <td>0.395078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.160534</td>\n",
              "      <td>0.400667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.112009</td>\n",
              "      <td>0.334678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.119759</td>\n",
              "      <td>0.346062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.135851</td>\n",
              "      <td>0.368580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.113682</td>\n",
              "      <td>0.337167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.097174</td>\n",
              "      <td>0.311727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.103004</td>\n",
              "      <td>0.320943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.097695</td>\n",
              "      <td>0.312562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.094106</td>\n",
              "      <td>0.306766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.129430</td>\n",
              "      <td>0.359764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.117331</td>\n",
              "      <td>0.342537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.115301</td>\n",
              "      <td>0.339560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.099310</td>\n",
              "      <td>0.315135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.101909</td>\n",
              "      <td>0.319231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.099328</td>\n",
              "      <td>0.315164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.095292</td>\n",
              "      <td>0.308693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.106726</td>\n",
              "      <td>0.326689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.149863</td>\n",
              "      <td>0.387122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.095878</td>\n",
              "      <td>0.309642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.107753</td>\n",
              "      <td>0.328257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.096697</td>\n",
              "      <td>0.310961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.121022</td>\n",
              "      <td>0.347883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.094509</td>\n",
              "      <td>0.307423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.093461</td>\n",
              "      <td>0.305713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.105452</td>\n",
              "      <td>0.324733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.107400</td>\n",
              "      <td>0.327719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.106416</td>\n",
              "      <td>0.326214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.099181</td>\n",
              "      <td>0.314930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.098076</td>\n",
              "      <td>0.313172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.096135</td>\n",
              "      <td>0.310057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.099537</td>\n",
              "      <td>0.315496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.111810</td>\n",
              "      <td>0.334379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.116128</td>\n",
              "      <td>0.340775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.095773</td>\n",
              "      <td>0.309473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.114306</td>\n",
              "      <td>0.338091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.114603</td>\n",
              "      <td>0.338531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.112293</td>\n",
              "      <td>0.335102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.093555</td>\n",
              "      <td>0.305868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.142300</td>\n",
              "      <td>0.101925</td>\n",
              "      <td>0.319257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.142300</td>\n",
              "      <td>0.111657</td>\n",
              "      <td>0.334152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.142300</td>\n",
              "      <td>0.104813</td>\n",
              "      <td>0.323749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.142300</td>\n",
              "      <td>0.094182</td>\n",
              "      <td>0.306890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.142300</td>\n",
              "      <td>0.132534</td>\n",
              "      <td>0.364052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.142300</td>\n",
              "      <td>0.144423</td>\n",
              "      <td>0.380030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.142300</td>\n",
              "      <td>0.092429</td>\n",
              "      <td>0.304021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.142300</td>\n",
              "      <td>0.095647</td>\n",
              "      <td>0.309269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.142300</td>\n",
              "      <td>0.094677</td>\n",
              "      <td>0.307696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.142300</td>\n",
              "      <td>0.095246</td>\n",
              "      <td>0.308619</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-10\n",
            "Configuration saved in ./tmp/checkpoint-10/config.json\n",
            "Model weights saved in ./tmp/checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-10/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-240] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-20\n",
            "Configuration saved in ./tmp/checkpoint-20/config.json\n",
            "Model weights saved in ./tmp/checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-20/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-410] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-30\n",
            "Configuration saved in ./tmp/checkpoint-30/config.json\n",
            "Model weights saved in ./tmp/checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-30/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-420] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-40\n",
            "Configuration saved in ./tmp/checkpoint-40/config.json\n",
            "Model weights saved in ./tmp/checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-40/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-430] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-50\n",
            "Configuration saved in ./tmp/checkpoint-50/config.json\n",
            "Model weights saved in ./tmp/checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-50/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-440] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-60\n",
            "Configuration saved in ./tmp/checkpoint-60/config.json\n",
            "Model weights saved in ./tmp/checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-60/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-450] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-70\n",
            "Configuration saved in ./tmp/checkpoint-70/config.json\n",
            "Model weights saved in ./tmp/checkpoint-70/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-70/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-70/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-460] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-80\n",
            "Configuration saved in ./tmp/checkpoint-80/config.json\n",
            "Model weights saved in ./tmp/checkpoint-80/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-80/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-80/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-470] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-90\n",
            "Configuration saved in ./tmp/checkpoint-90/config.json\n",
            "Model weights saved in ./tmp/checkpoint-90/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-90/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-90/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-480] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-100\n",
            "Configuration saved in ./tmp/checkpoint-100/config.json\n",
            "Model weights saved in ./tmp/checkpoint-100/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-100/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-490] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-110\n",
            "Configuration saved in ./tmp/checkpoint-110/config.json\n",
            "Model weights saved in ./tmp/checkpoint-110/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-110/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-110/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-500] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-120\n",
            "Configuration saved in ./tmp/checkpoint-120/config.json\n",
            "Model weights saved in ./tmp/checkpoint-120/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-120/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-120/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-510] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-130\n",
            "Configuration saved in ./tmp/checkpoint-130/config.json\n",
            "Model weights saved in ./tmp/checkpoint-130/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-130/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-130/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-520] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-140\n",
            "Configuration saved in ./tmp/checkpoint-140/config.json\n",
            "Model weights saved in ./tmp/checkpoint-140/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-140/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-140/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-530] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-150\n",
            "Configuration saved in ./tmp/checkpoint-150/config.json\n",
            "Model weights saved in ./tmp/checkpoint-150/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-150/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-150/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-540] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-160\n",
            "Configuration saved in ./tmp/checkpoint-160/config.json\n",
            "Model weights saved in ./tmp/checkpoint-160/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-160/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-160/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-550] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-170\n",
            "Configuration saved in ./tmp/checkpoint-170/config.json\n",
            "Model weights saved in ./tmp/checkpoint-170/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-170/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-170/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-560] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-180\n",
            "Configuration saved in ./tmp/checkpoint-180/config.json\n",
            "Model weights saved in ./tmp/checkpoint-180/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-180/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-180/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-570] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-190\n",
            "Configuration saved in ./tmp/checkpoint-190/config.json\n",
            "Model weights saved in ./tmp/checkpoint-190/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-190/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-190/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-580] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-200\n",
            "Configuration saved in ./tmp/checkpoint-200/config.json\n",
            "Model weights saved in ./tmp/checkpoint-200/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-200/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-200/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-590] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-210\n",
            "Configuration saved in ./tmp/checkpoint-210/config.json\n",
            "Model weights saved in ./tmp/checkpoint-210/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-210/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-210/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-10] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-220\n",
            "Configuration saved in ./tmp/checkpoint-220/config.json\n",
            "Model weights saved in ./tmp/checkpoint-220/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-220/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-220/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-20] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-230\n",
            "Configuration saved in ./tmp/checkpoint-230/config.json\n",
            "Model weights saved in ./tmp/checkpoint-230/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-230/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-230/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-30] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-240\n",
            "Configuration saved in ./tmp/checkpoint-240/config.json\n",
            "Model weights saved in ./tmp/checkpoint-240/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-240/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-240/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-40] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-250\n",
            "Configuration saved in ./tmp/checkpoint-250/config.json\n",
            "Model weights saved in ./tmp/checkpoint-250/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-250/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-250/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-50] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-260\n",
            "Configuration saved in ./tmp/checkpoint-260/config.json\n",
            "Model weights saved in ./tmp/checkpoint-260/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-260/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-260/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-60] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-270\n",
            "Configuration saved in ./tmp/checkpoint-270/config.json\n",
            "Model weights saved in ./tmp/checkpoint-270/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-270/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-270/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-70] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-280\n",
            "Configuration saved in ./tmp/checkpoint-280/config.json\n",
            "Model weights saved in ./tmp/checkpoint-280/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-280/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-280/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-80] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-290\n",
            "Configuration saved in ./tmp/checkpoint-290/config.json\n",
            "Model weights saved in ./tmp/checkpoint-290/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-290/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-290/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-90] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-300\n",
            "Configuration saved in ./tmp/checkpoint-300/config.json\n",
            "Model weights saved in ./tmp/checkpoint-300/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-300/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-300/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-100] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-310\n",
            "Configuration saved in ./tmp/checkpoint-310/config.json\n",
            "Model weights saved in ./tmp/checkpoint-310/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-310/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-310/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-110] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-320\n",
            "Configuration saved in ./tmp/checkpoint-320/config.json\n",
            "Model weights saved in ./tmp/checkpoint-320/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-320/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-320/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-120] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-330\n",
            "Configuration saved in ./tmp/checkpoint-330/config.json\n",
            "Model weights saved in ./tmp/checkpoint-330/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-330/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-330/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-130] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-340\n",
            "Configuration saved in ./tmp/checkpoint-340/config.json\n",
            "Model weights saved in ./tmp/checkpoint-340/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-340/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-340/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-140] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-350\n",
            "Configuration saved in ./tmp/checkpoint-350/config.json\n",
            "Model weights saved in ./tmp/checkpoint-350/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-350/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-350/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-150] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-360\n",
            "Configuration saved in ./tmp/checkpoint-360/config.json\n",
            "Model weights saved in ./tmp/checkpoint-360/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-360/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-360/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-160] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-370\n",
            "Configuration saved in ./tmp/checkpoint-370/config.json\n",
            "Model weights saved in ./tmp/checkpoint-370/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-370/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-370/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-170] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-380\n",
            "Configuration saved in ./tmp/checkpoint-380/config.json\n",
            "Model weights saved in ./tmp/checkpoint-380/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-380/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-380/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-180] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-390\n",
            "Configuration saved in ./tmp/checkpoint-390/config.json\n",
            "Model weights saved in ./tmp/checkpoint-390/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-390/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-390/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-190] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-400\n",
            "Configuration saved in ./tmp/checkpoint-400/config.json\n",
            "Model weights saved in ./tmp/checkpoint-400/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-400/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-400/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-200] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-410\n",
            "Configuration saved in ./tmp/checkpoint-410/config.json\n",
            "Model weights saved in ./tmp/checkpoint-410/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-410/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-410/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-210] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-420\n",
            "Configuration saved in ./tmp/checkpoint-420/config.json\n",
            "Model weights saved in ./tmp/checkpoint-420/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-420/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-420/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-220] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-430\n",
            "Configuration saved in ./tmp/checkpoint-430/config.json\n",
            "Model weights saved in ./tmp/checkpoint-430/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-430/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-430/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-230] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-440\n",
            "Configuration saved in ./tmp/checkpoint-440/config.json\n",
            "Model weights saved in ./tmp/checkpoint-440/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-440/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-440/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-240] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-450\n",
            "Configuration saved in ./tmp/checkpoint-450/config.json\n",
            "Model weights saved in ./tmp/checkpoint-450/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-450/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-450/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-250] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-460\n",
            "Configuration saved in ./tmp/checkpoint-460/config.json\n",
            "Model weights saved in ./tmp/checkpoint-460/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-460/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-460/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-260] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-470\n",
            "Configuration saved in ./tmp/checkpoint-470/config.json\n",
            "Model weights saved in ./tmp/checkpoint-470/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-470/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-470/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-270] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-480\n",
            "Configuration saved in ./tmp/checkpoint-480/config.json\n",
            "Model weights saved in ./tmp/checkpoint-480/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-480/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-480/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-280] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-490\n",
            "Configuration saved in ./tmp/checkpoint-490/config.json\n",
            "Model weights saved in ./tmp/checkpoint-490/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-490/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-490/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-290] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-500\n",
            "Configuration saved in ./tmp/checkpoint-500/config.json\n",
            "Model weights saved in ./tmp/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-500/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-300] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-510\n",
            "Configuration saved in ./tmp/checkpoint-510/config.json\n",
            "Model weights saved in ./tmp/checkpoint-510/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-510/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-510/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-310] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-520\n",
            "Configuration saved in ./tmp/checkpoint-520/config.json\n",
            "Model weights saved in ./tmp/checkpoint-520/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-520/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-520/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-320] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-530\n",
            "Configuration saved in ./tmp/checkpoint-530/config.json\n",
            "Model weights saved in ./tmp/checkpoint-530/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-530/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-530/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-330] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-540\n",
            "Configuration saved in ./tmp/checkpoint-540/config.json\n",
            "Model weights saved in ./tmp/checkpoint-540/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-540/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-540/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-340] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-550\n",
            "Configuration saved in ./tmp/checkpoint-550/config.json\n",
            "Model weights saved in ./tmp/checkpoint-550/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-550/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-550/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-360] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-560\n",
            "Configuration saved in ./tmp/checkpoint-560/config.json\n",
            "Model weights saved in ./tmp/checkpoint-560/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-560/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-560/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-350] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-570\n",
            "Configuration saved in ./tmp/checkpoint-570/config.json\n",
            "Model weights saved in ./tmp/checkpoint-570/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-570/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-570/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-370] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-580\n",
            "Configuration saved in ./tmp/checkpoint-580/config.json\n",
            "Model weights saved in ./tmp/checkpoint-580/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-580/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-580/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-380] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./tmp/checkpoint-590\n",
            "Configuration saved in ./tmp/checkpoint-590/config.json\n",
            "Model weights saved in ./tmp/checkpoint-590/pytorch_model.bin\n",
            "tokenizer config file saved in ./tmp/checkpoint-590/tokenizer_config.json\n",
            "Special tokens file saved in ./tmp/checkpoint-590/special_tokens_map.json\n",
            "Deleting older checkpoint [tmp/checkpoint-390] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./tmp/checkpoint-560 (score: 0.0924287959933281).\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: kfold, t1, t3, t2.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 197\n",
            "  Batch size = 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: t1, t3, t2.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "####################################################################################################\n",
            "FOLD 4 SCORE: 0.30402109026908875\n",
            "####################################################################################################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yFhayW1SCTD",
        "outputId": "d166e724-1041-4b79-d9bf-92d5d20425c2"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'fold': 0, 'rmse': 0.3217563331127167, 'seed': 0},\n",
              " {'fold': 1, 'rmse': 0.30787310004234314, 'seed': 0},\n",
              " {'fold': 2, 'rmse': 0.2834027111530304, 'seed': 0},\n",
              " {'fold': 3, 'rmse': 0.31405699253082275, 'seed': 0},\n",
              " {'fold': 4, 'rmse': 0.30402109026908875, 'seed': 0}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hsxdd5EAM9m"
      },
      "source": [
        "preds = np.stack(preds)\n",
        "mean_preds = preds.mean(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10Quett1wnjw"
      },
      "source": [
        "tst['score'] = mean_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Kyjc4fITyGc-",
        "outputId": "740468a8-6f65-4f7e-ab70-e55393612b19"
      },
      "source": [
        "tst.score.hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2184210cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hc-wME53YgY",
        "outputId": "d99a8f10-cfaf-47f3-d5a9-f3077666855a"
      },
      "source": [
        "print_high_low(tst)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Worst translation:\n",
            "Translation: Oprócz uczynienia tego osobistym, więc będziemy rozmawiać o twoim związku z twoim sercem, a wszystkie kobiety to związek z ich sercem, będziemy wnosić w politykę.\n",
            "Reference: Oprócz tego że jest to sprawa osobista porozmawiamy o waszym podejściu do serca i ogólnie o podejściu kobiet do ich serca. Porozmawiamy o polityce\n",
            "Best tranlation:\n",
            "Translation: Zacznijmy od obejrzenia kilku świetnych zdjęć.\n",
            "Reference: Zacznijmy od obejrzenia kilku wspaniałych zdjęć.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk8Z5jHc4h4X"
      },
      "source": [
        "out = tst['score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4c7BLfu_Xe9"
      },
      "source": [
        "out.to_csv(f'drive/MyDrive/poleval2021/nonblind_{EXPNUM}.tsv', sep='\\t', index=False, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mLHdWvd_1Mv"
      },
      "source": [
        "# out2 = tst['score'].apply(lambda r: np.round(r, decimals=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyFEeM7IKT39"
      },
      "source": [
        "# out2.to_csv(f'drive/MyDrive/poleval2021/nonblind_{EXPNUM}a.tsv', sep='\\t', index=False, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxPWUJoz39xp",
        "outputId": "e4b78885-82b2-4a22-d68a-3693350bdc87"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvmGj5jIKYhe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}